%\section{Description}
\label{quick}

Quicksort is an in-place, unstable sort. It was first described in
\cite{HoarePartition61} and \cite{HoareQuicksort61}, and later in more detail in
\cite{Hoare62}. Sedgewick did his PhD on quicksort, and several important
improvements to quicksort are due to this. They are discussed in
\cite{Sedgewick78} and \cite{Sedgewick02}. \cite{BentleyMcIlroy93} discusses
considerations for a fast C implementation. \cite{SedgewickBentley02} discusses
equal keys and proves that quicksort has the lowest complexity of any
comparison-based sort.

Quicksort is an algorithm of the type known as \n{divide and conquer}. It
recursively partitions its array around a pivot, swapping large keys on
the left hand side with smaller keys on the right hand side. The partition
causes the pivot to be put in its final position, with all greater keys to the
right, and lesser keys on the left. The left and right partitions are then
sorted recursively.

The recursive nature of quicksort means that even though quicksort is in-place,
there is a requirement of a stack. In the worst case, this stack can grow to
$O(N)$, if the largest or smallest key is always chosen as the pivot. This
worst case will occur frequently, especially when trying to sort an already
sorted, or almost sorted list.

Figure \vref{Simple quicksort implementation} contains a simple implementation of
the quicksort algorithm.

\code{Simple quicksort implementation}{simple_quicksort.c}

\section{Base Quicksort}

LaMarca uses the optimized quicksort described by Sedgewick for his base
quicksort. This is an iterative quicksort which uses an explicit stack. In the
worst case, this stack must be the same size as the array, and must be
initialised at the start of the algorithm. To reduce the size of the stack,
Sedgewick proposes sorting the smallest partition first. This optimization
reduces the maximum size of the stack to $O(logN)$. In practice, two \cc{int}s,
the left and the right index, are required for each partition.

Sedgewick's second improvement is the use of an elementary sort for small
sub-files.  Once a partition is smaller than an arbitrary threshold, insertion
sort is used to sort the partition. Sedgewick states that roughly the same
results are obtained for thresholds between 5 and 25. An additional reduction in
instruction count can be had by doing the insertion sort over the entire array
at the end, rather than across each small array as soon as the threshold is
reached. At the end of the partition sorting, LaMarca places a sentinel at the
end of the array and uses an insertion sort, eliminating the bounds check from
the end. In this implementation, the sentinel is placed at the front, and the
smallest of the first \cc{THRESHOLD} keys is used as a sentinel, in keeping
with our policy of not exceeding the bounds of the array.

Choosing a pivot which is close to the median of the keys to be sorted ensures
quicksort achieves optimal performance. Hoare suggested a random pivot, but the
added cost of a random number generator would severely slow the algorithm.
Sedgewick recommends that the median of three keys is chosen, the three keys
being the first, middle and last in the list. This reduces the chance of a worst
case occurring significantly, notably in the case of sorted and almost sorted
lists. 

An additional benefit of Sedgewick's median-of-3 implementation is that it
provides a sentinel for the partition. It is now possible to remove the bounds
check from the innermost loops of \cc{partition}, saving a lot of instructions.

Between these three improvements, Sedgewick estimates a reduction in
instruction count of roughly 30\%. The size of the auxiliary stack is also
reduced significantly, and the possibility of stack overflow, in the worst case
of the recursive version, is removed.

Quicksort has been well studied, and several important improvements have been
noted. In \cite{BentleyMcIlroy93}, using a median-of-9 partition is discussed.
This is discussed below. Modern discussions on quicksort refer to the problem of
equal keys, which is discussed there, as well as by Sedgewick in
\cite{SedgewickBentley02}. These improvements are not included here for several
reasons. Firstly, a quicksort which takes into account equal keys puts the equal
keys at the front of the array, then copies them into place later. These copies
would represent a lot of cache misses, and the results may overwhelm the
improvements LaMarca makes in the next phase of the algorithm. Secondly, when
sorting random keys over a large range, the number of equal keys will be very
low. Using a median of greater than three is discussed in Section
\ref{quick-predictors2}.

\section{Memory-tuned Quicksort}

LaMarca's first step was to increase the temporal locality of the algorithm by
performing insertion sort as soon as the partitioning stops. Once a partition is
reduced below the size of the threshold, insertion sort is preformed
immediately. By doing this, the keys being sorted are in the cache already. By
leaving the insertion sort until the end, as Sedgewick does, the reduction in
instruction count is made up by the increase in cache misses, once the
lists being sorted are larger than the cache. 

As discussed above, it is necessary to insert either a bounds check or a
sentinel. If the partition to be sorted is the left most partition, then the
smallest key in the partition will be chosen as the sentinel. No other partition
needs a sentinel. A possible way to avoid the sentinel in that partition is to
place zero at $0^{th}$ position in the list to be sorted, and sort whichever
number was in this position back at the end. This sort could be done with one
iteration of insertion sort, and would have just one branch miss.  However, the
number of level 2 cache misses would be great in this case, and the cost of this
would be substantial. This could be reduced by choosing the key to remove from a
small list, such as the eight keys at the start, middle and end of the array.
This would introduce a constant number of cache misses, but would significantly
reduce the chances of having to sort across the entire array at the end. This,
however, is not implemented; instead a check is done to fetch a sentinel from
the appropriate sub-file if required.  This check isn't very expensive, and is
significantly cheaper than either a bounds check or putting a sentinel into
every sub-file.

\section{Multi-Quicksort}

In the case that the array to be sorted fits in a particular level of
the cache, then quicksort has excellent cache properties at that
level. Once the array is larger than the cache, however, the
number of misses increases dramatically. To fix this problem, LaMarca uses a
technique similar to that used in multi-mergesort. Instead of multi-merging a
series of sorted lists at the end, a \n{multi-way partition} is done at the
start, moving all keys into cache sized containers which can be sorted
individually.

At the start of the algorithm, a set of pivots is chosen and sorted, and keys
are put into containers based on which two pivots their value lies between. A
minimal and maximal key are put on the edges of this list to avoid bounds checks
as the list is iterated over. Because of the way the pivots were chosen, two
pivots which happen to be close together would result in a very small number of
keys being put into their container. While this is not a bad thing, this will
result in a larger number of keys in another container. Conversely, a large
space between two consecutive pivots will mean that the keys in this
container will not all fit into the cache at once.

There are two possible solutions to this. The first is to choose the pivots
well, so that they are very evenly distributed. The second is to choose the
number of pivots so that the average size of the containers is not larger than
the cache. LaMarca presents calculations showing that by choosing the average
subset size to be $C/3$, where $C$ is the number of keys which fit in the
cache, on average only 5\% of subsets will be larger than the cache.

Since the maximum number of keys which must fit in a container is not known, it
is necessary to implement containers using linked lists. Each list contains a
block of a few thousand keys. LaMarca notes very little performance difference
between 100 and 5,000 keys in each block. The size is therefore chosen so that
each list fits exactly into a page of memory. When a list is filled, another
list is linked to it, and these lists hold all the keys in the container.

The initial implementation of this allocated a new list every time an old one
was filled. This lowers performance significantly, as it results in a lot of
system calls, which increase instruction count and cache misses. Instead, it is
possible to work out the maximum possible number of lists required to create the
containers, and allocate them at the start. When a list runs out of space, it is
linked to an unused list from the large allocation. This wastes space, since it
is almost guaranteed that less lists will be required than available, though the
number of wasted lists allocated will be less than the number of pivots, and
therefore not exceptionally high.

The entire array is iterated though a key at a time. For each key, the list
of pivots are searched until the appropriate container is found, and the key is
added to that container. Sequentially searching the lists increases the
instruction count by 50\% over the memory-tuned version. For this reason, the
search was replaced with an efficient binary search, which reduced by half the
extra instructions.

Once the entire array has been put into containers, each container is emptied
in turn and the keys put back into the array. When a container is empty, the
pivot greater than that container is put into position, which is guaranteed to
be its final position.  The indices of these positions is pushed onto the
stack, and the sub-array is then sorted.

While emptying the containers back into the array, an opportunity is taken to
find the smallest key in the leftmost partition. This is placed as a sentinel,
ensuring that every partition has a sentinel, and that no bounds checks are
required.

\section{Results}

\subsection{Test Parameters}

When the size of a partition to be sorted is less than a certain
threshold value, insertion sort is used instead of quicksort. The
chosen threshold was 10, simply because this is the number used in
Sedgewick's book.

The number of keys in the linked list is 1022 since a word is needed for the
count, and another as a pointer to the next list. 1024 is the number of 32-bit
words which fit in a 4KB page of memory. In earlier versions where each page was
\cc{malloc}ed individually, the number of keys in the linked list was 1018,
as 4 words were used by the particular version of \cc{malloc} used by
SimpleScalar\footnote{This particular \cc{malloc} forms part of
\n{glibc}, the GNU C library, version 1.09.}.

A 2MB cache was assumed, so the number of 32 bit integer keys which fit in the
cache for this in-place sort is 524,288. Multi-quicksort began once the number
of keys was larger than 174,762.

\subsection{Expected Performance}

The instruction count for each quicksort is expected to be $O(NlogN)$, though
not as high as the other algorithms due to the efficient inner loop.  The number
of instructions for multi-quicksort should increase by about 50\% in the case of
the sequential sort, and 25\% in the case of the binary sort. Memory-tuned
quicksort should also have a slightly higher instruction count than base
quicksort, due to the change in use of insertion sort.

LaMarca notes that quicksort is already very cache efficient as it has excellent
spatial and temporal locality. Once it no longer fits in the cache, however, the
number of misses should increase significantly. This is especially true in the
case of the base quicksort, which does an insertion sort across then entire
array at the end.

Memory-tuned quicksort should have fewer misses than base quicksort as soon as
the array is larger than the cache. It is expected that these misses will make
up for the increase in instruction count.

LaMarca estimates a large reduction in level 2 misses in multi-quicksort, and
puts a figure on it of four misses per cache block, meaning, in our case, half a
miss per key.

The number of branches should increase in multi-quicksort along with the
instruction count. When a binary search is used, the increase in mispredictions
is expected to be significant. With a sequential search, however, the number of
misses should not increase at all, and should flat-line from that point on.

\subsection{Simulation Results}

\plot{quick}{}{}{Results shown are for a direct-mapped cache.}{Results shown are
for a direct-mapped cache.}{}{}{}

The instruction count, shown in Figure \ref{quick instructions} is mostly as
predicted. The increase in instructions in both multi-quicksorts is as
predicted, with the binary search version increasing only 25\% against the
sequential search version's 50\% increase. However, the increase we predicted in
the instruction count of memory-tuned quicksort based on the results of both Sedgewick
and LaMarca -  that is, once the efficient insertion sort was replaced with many
small insertion sorts - we observe that the instruction count remains exactly
the same.

The cache results in Figure \ref{quick level 2 misses} were similar to
predictions, except that the multi-quicksorts did not behave as expected, and
performed as badly as the memory-tuned version. However, the rate at which the
cache misses are increasing is very low, being almost a flat line on our graph.
LaMarca's calculation of four misses per cache block is very accurate, as we
show 0.55 misses per key, or 4.4 misses per block.

The memory-tuned quicksort performed better than the base version, as predicted.
Both the memory-tuned version and the base quicksort both have a significant
increase once the arrays no longer fit in the cache, also as predicted.

The results show again (see Section \ref{mergesort replacement policy}) that
direct-mapped caches can perform better than fully-associative ones. This is
true in the case of all the quicksorts, as can be seen in Figures \ref{base
quicksort (no median) level 2 misses} to \ref{multi-quicksort (sequential
search) level 2 misses} graph that this is true in the case of all the
quicksorts. As in Section \ref{mergesort replacement policy}, this is due to the
replacement policy of the fully-associative cache.

The branch prediction results are shown in Figure \ref{quick branch misses}. As
expected, there was a a dramatic increase in the binary search multi-quicksort
branch misses, but only when using a bimodal predictor. When using a two-level
adaptive predictor, the number of misses is, in general, slightly higher, but it
does not spike due to the binary search. While it performs worse than the
bimodal predictor, it appears that two-level adaptive predictors can adapt to
the erratic branches of a binary search.

The sequential search stops the increase of branch misses, since every key will
only take one miss to find its proper place. The results of this on cycle
count, shown in Figure \ref{quick cycles} are enough to make up for the increase
in instruction count, which was significantly reduced by using the binary
search.

Memory-tuned quicksort had fewer branch misses than base quicksort, due to the fact
that fewer comparative branches are executed. In base quicksort, we sort the
entire array using insertion sort. In tuned quicksort, we don't sort the pivots,
but only the keys between them.

Despite the sequential search multi-quicksort performing better than its
binary searched cousin, and its good cache performance, it still failed to
approach the performance of either base or tuned quicksort. In fact, base
quicksort was the fastest of all the quicksorts in our performance counter test,
even though our simulator results show that it has more level 1 misses, level 2
misses, and branch misses than tuned quicksort, and the same number of
instructions.

LaMarca's results are again similar to the results presented here, but not
identical. The shape of the instruction count graph is the same in both cases,
but that of the cache misses is not. The ratio of the cache misses between base
and memory-tuned quicksort are the same in both sets of results, but these are lower
than the same misses in LaMarca's results. However, the multi-quicksort results
appear to be the same, though our results are not conclusive either way.


\section{A More Detailed Look at Branch Prediction}

%TODO
% \textbf{[David] I'm not sure what to do with this section. Part of the
% problem is that the pictures are very difficult to understand. Another
% problem is that we are trying to show differences that are almost
% invisible. The median of 9 picture should definitely be removed from
% this subsection. However, I suspect that the whole of this subsection should
% go. What do we want to say? One really puzzling result is that there
% seem to be no median branches at all for cache quicksort. This looks
% strikingly like a programming error.}

% \textbf{[David] On thing that's badly needed in this section is
% a detailed description of what these funny charts all mean. However,
% the appropriate place for most of this is the place where the first of
% these charts appeared.}


\sixplot{base_quicksort__median_of_3__0.eps}{memory_tuned_quicksort_0.eps}{multi_quicksort__binary_search__0.eps}{multi_quicksort__sequential_search__0.eps}{}{}{base,
memory-tuned and multi- quicksorts}

Figure \ref{base, memory-tuned and multi- quicksorts-1} shows the behaviour of
the indiviual branches in base quicksort. The \n{i} branch is the loop control
branch for the index sweeping from left-to-right up the array during partition.
The \n{j} branch is the corresponding loop sweeping down the array. The
partition step terminates when \n{i $\geq$ j}, and the \n{Partition End} branch
checks that condition. The \n{Insertion} branch is the comparison branch in the
insertion sort algorithm. Recall that our quicksort algorithm does not sort
partitions which are smaller than 10 elements, but instead does a final pass of
insertion sort over the whole array to sort these small partitions. Finally, the
\n{Median} branch is the sum of the results for the branches involved in
computing the median-of-three for each partition.

The only difference between memory-tuned quicksort and base quicksort is that
memory-tuned quicksort applies insertion sort to small partitions immediately,
whereas base quicksort does a single insertion sort pass at the end. The result
is that both algorithms perform almost identically from the viewpoint of branch
prediction. Memory-tuned quicksort uses a very slightly smaller number of
insertion sort comparison branches, because it sorts only within partitions and
does not attempt to move array elements that have acted as pivots. (The reason
that quicksort works is that after partitioning, the pivot is in its correct
location, and must never move again). In practice, this reduction in branches is
probably offset by the increase in mispredictions in the outer loop exit branch
of the many insertion sorts.

Base quicksort has a 35\% branch misprediction rate, whereas the rate for
memory-tuned quicksort is only 34\%. The main reason for this is the reduction
in branches by not attempting to move the pivot elements.  Attempting to move
these elements usually results in a branch misprediction. The main reason is
that most other elements move at least one position, and thus cause the
insertion sort loop to iterate at least once. But the pivot elements never move
-- they are already in the correct place.

When we look at multi-quicksort, there is a 19\% drop in the number of
iterations of the \n{i} and \n{j} loops, with a corresponding reduction in the
number of mispredictions. Interestingly, the percentage misprediction rate of
the \n{i} and \n{j} branches does not change -- it is 37\% in both cases.


% Once the sort is larger than the cache, the number fo steps increases linearly
% (since it does a sort of counting like radix sort). Theres a decrease of 19 \%.
% Since the sort at this point is NlogM where M is the size of the cache (in
% keys), there shouldnt be a change in misprediction rates. And there sint.
% Before: 63\%. After, 63\%. So no difference.

Multi-quicksort does, however, introduce a lot of additional branches. We
already saw in Figure \ref{quick instructions}, that the binary search and
sequential search variants increase the number of executed instructions by
around 25\% and 50\% respectively. Figure \ref{base, memory-tuned and multi-
quicksorts-3} shows additional measures of the \n{Binary Left} and \n{Binary
Right} branches, which are the branches within the binary search code. The
number of binary search branches is much greater than the reduction in \n{i}
and \n{j} branches. Furthermore, these binary search branches are highly (43\%)
unpredictable.

The variation of multi-quicksort that uses sequential search dramatically
increases the number of executed branches, but these branches are highly
predictable. This version actually causes fewer branch mispredictions than
memory-tuned quicksort.

% Theres a lot of branches in the multi quicksort. WE saw in other results that
% the mere instruction cost of this bogs down to algorithm significantly. in the
% binary search version, there are more than twice as many mispredictions,
% despite the fact that there are less than half the branches. (note that i dont
% think this was a wonderful binary search, its heavily biased towards the
% checking the left). binary search predicts correctly 57 percent of the time,
% meaning that it mostly changes direction haphazardly during the search. No
% wonder it performs so badly.

% Note that sequential multi-quicksort has less mispredictions than cache quicksort.
\subsection{Different Medians}

\label{quick-predictors2}
\sixplot{base_quicksort__no_median__0.eps}{base_quicksort__median_of_3__0.eps}{base_quicksort__pseudo_median_of_5__0.eps}{base_quicksort__pseudo_median_of_7__0.eps}{base_quicksort__pseudo_median_of_9__0.eps}{}{base quicksort without a median, with median of 3, and with pseudo-medians of 5, 7 and 9}

\label{quicksort entropy}
When we originally looked at quicksort, we expected that the \n{i} and \n{j}
branches in quicksort would resolve in each direction. In fact, this is not the
case. The main reason is that the chosen pivot is almost never the true median
of the section of the array to be partitioned. Thus, the \n{i} and \n{j}
branches are almost always biased, which makes them predictable. According to
Knuth, if the pivot element is chosen randomly, then the \n{i} and \n{j}
branches will be taken (i.e.  the \n{i} or \n{j} loop will execute) twice as
often as they are not taken (i.e. the loop exits).

In order to investigate this, we implemented a simple version of quicksort that
uses the middle element as the pivot rather than a median-of-three\footnote{It
is important to note that selecting the pivot based on only a single element can
be particularly inefficient because it removes the natural sentinels at the top
and bottom of the regions to be partitioned that we get from median-of-three. In
Figure \ref{quick-predictors2} we see that this results in an additional branch
to handle the edgecase without a sentinel. Although this branch is almost 100\%
predictable, it is executed a very large number of times.}. The arrays we sort
are random, so the value of this element will also be random. Figure \ref{base
quicksort without a median, with median of 3, and with pseudo-medians of 5, 7
and 9-1} shows the behaviour of each of the important branches in this code. We
see that the \n{i} and \n{j} branches are indeed taken about twice as often as
not-taken, and the \n{i} and \n{j} branches are correctly predicted around 71\%
of the time.

The result that the branches are more predictable (71\%) than they are biased
(66\%) is a particularly interesting finding. The reason is that the bias in the
results is an average figure over a large number of partitions that quicksort
performs to sort the array. On a particular partition, the bias will be more
pronounced because the \n{i} and \n{j} branches will be biased in opposite
directions.

For example, consider the case where the pivot element is larger than the true
median. In this case, the final resting place of the pivot will be towards the
right-hand end of the region to be partitioned. Thus, the \n{i} loop will
execute a large number of times, and thus the \n{i} loop will be biased towards
being taken. The \n{j} loop, on the other hand, will be biased toward not-taken,
because the average element is likely to be smaller than the pivot.  Therefore,
the \n{j} loop will execute only a small number of times. Thus, we see that in a
particular partition, we do not normally get an equal number of executions of
the \n{i} and \n{j} loops. Instead we get a large number of executions in one
loop (which makes its branch relatively predictable), whereas the other loop
often does not enter at all (which makes its branch also predictable, although
executed a much smaller number of times). Thus, although overall the \n{i} and
\n{j} branches are taken twice as often as not-taken, on a particular partition
one will usually be biased in the not-taken direction, and as a result will be
executed a relativelty small number of times.

In the preceding paragraphs we have seen that the \n{i} and \n{j} branches are
predictable when we have a pivot that is larger or smaller than the true median
of the array section to be partitioned. An interesting question is the effect of
choosing a better approximation of the true median on branch prediction. Figure
\ref{base quicksort without a median, with median of 3, and with pseudo-medians
of 5, 7 and 9-2} shows the behavior of these branches when median-of-three is
used to select the pivot. Perhaps the biggest effect is that the number of
executions of the \n{i} and \n{j} branches falls by around 15\%. This fall is
partly offset by the comparison branches to compute the median, but these
account for only around 1.25\% of the original branches from the no-median
quicksort. Thus, there is a reduction of almost 14\% in the number of executed
comparison branches. Even more interesting, however, is the bias in the
remaining \n{i} and \n{j} branches. The no-median \n{i} and \n{j} branches are
66\% biased, but the median-of-three branches are only 60\% biased in the taken
direction. The accuracy with which these branches are correctly predicted falls
from 71\% to 64\%.

The reason for the reduction in bias and prediction accuracy is that
median-of-three gives a much better estimate of the true median of the section
to be partitioned. The final resting place of the pivot will usually be closer
to the middle of the section, to there is less bias in the branches. Thus,
although median-of-three gives around a 14\% reduction in executed comparison
branches, each of those branches is much less likely to be correctly predicted.
Overall there is actually a 6\% \n{increase} in the total number of branch
mispredictions.

To further investigate this phenomenon, we measured the effects of using
pseudo-medians-of-five, -seven and -nine to choose the pivot. A pseudo-median
uses repeated medians-of-three for parts of the computation to reduce the number
of comparisons in computing the pivot. The reductions in executed comparison
branches are 16.5\%, 17.8\% and 19.6\% respectively. However, the biases of the
$i$ and $j$ branches in these algorithms are only 57.8\%, 56.3\% and 55.1\%. The
corresponding predictabilities are 61.6\%, 59.9\% and 58.3\%. Overall, the
increases in comparison branch mispredictions (including computing the
pseudo-median) are 9.2\%, 11.8\% and 14.3\%. 

Picking a better estimate of the true median substantially reduces the number of
executed branches, but each of those branches is much more unpredictable.
Intuitively, this makes sense. Sorting the array involves putting an order on
it, and thus reducing its entropy. If we want to sort it in a smaller number of
steps, then each step must remove more randomness. Thus, we would expect that
the outcome of the branch that removes the randomness would itself be more
random.

Figure \ref{quick_median_cycles} shows the cycle count of each of the
quicksorts, showing that a pseduo-median-of-seven offers the greatest tradeoff
between the reduced number of branches and instructions, and the increase in
branch mispredictions.

An additional advantage of using a higher order median is that it dramatically
reduces the probability of quicksort exhibiting worst case ($O(N^2)$) time
complexity. Although the average case reduction in running time is small,
reducing the chance of worst-case performance, which might cause the program to
pause for minutes or hours, is very valuable.

\begin{figure}
\includegraphics{plots/quick_median_cycles.eps}
\caption{Empiric cycles count for quicksort with varying medians}
\label{quick_median_cycles}
\end{figure}


\section{Future Work}
The threshold in quicksort is the point at which a simpler sort is used to sort
the partition. A larger threshold will results in fewer checks for a sentinel. A
smaller threshold means that the instruction count due to the insertion sort
will be lower, but more partition steps may be taken. A smaller threshold also
makes it more likely that the keys to be sorted all fit in a single cache
block. However, the partition is likely to occur across a cache block boundary,
though this is not necessarily a bad thing; the next partition to be sorted will
be adjacent to the current block, and will already be in memory. Observing how
results, especially instruction count and branch misses, vary with changes 
to the threshold may be interesting.

Two areas of current research into quicksort are those of equal keys, and
median-of-greater-than-3. The former affects the algorithmic properties, and
some steps may need to be varied to avoid extra cache misses as a result
of extra copying. The latter simply affects the instruction count, though the
extra comparisons are likely to be mispredicted. These results could be
measured, giving greater insight to this debate.

Memory-tuned quicksort is the only algorithm here that requires a sentinel
check. The branch predictor will only mispredict this once, so the instruction
count increase is only that of removing a single check which occurs relatively
few ($N / THRESHOLD$) times\footnote{This is a minimum. The figure may be higher
since partitions are likely to be smaller than the threshold.}. Still, the
problem is interesting and has applications in other algorithm designs.

Finally, in multi-quicksort, it may be possible to choose better pivots.
Choosing better (ie, more evenly distributed pivots) would reduce the number of
containers. The advantage of this is that it reduces linearly the number of
pivots to be searched by the sequential search. The most this can be reduced by
is a factor of three, indicating perfectly distributed pivots. This would reduce
the instruction overhead of the multi-quicksort considerably.
