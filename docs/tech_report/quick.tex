%\section{Description}
\label{quick}

Quicksort is an in-place, unstable sort. It was first described in
\cite{HoarePartition61} and \cite{HoareQuicksort61}, and later in more detail in
\cite{Hoare62}. Sedgewick did his PhD on quicksort, and several important
improvements to quicksort are due to this. They are discussed in
\cite{Sedgewick78} and \cite{Sedgewick02}. \cite{BentleyMcIlroy93} discusses
considerations for a fast C implementation. \cite{SedgewickBentley02} discusses
equal keys and proves that quicksort has the lowest complexity of any comparison
based sort.

Quicksort is an algorithm of the type known as \n{divide and conquer}. It
recursively partitions its array around a pivot, swapping large keys on
the left hand side with smaller keys on the right hand side. The partition
causes the pivot to be put in its final position, with all greater keys to the
right, and lesser keys on the left. The left and right partitions are then
sorted recursively.

The recursive nature of quicksort means that even though quicksort is in-place,
there is a requirement of a stack. In the worst case, this stack can grow to
$O(N)$, if the largest or smallest key is always chosen as the pivot. This
worst case will occur frequently, especially when trying to sort an already
sorted, or almost sorted list.

Figure \vref{Simple quicksort implementation} contains a simple implementation of
the quicksort algorithm.

\code{Simple quicksort implementation}{simple_quicksort.c}

\section{Base Quicksort}

LaMarca uses the optimized quicksort described by Sedgewick for his base
quicksort. This is an iterative quicksort which uses an explicit stack. In the
worst case, this stack must be the same size as the array, and must be
initialised at the start of the algorithm. To reduce the size of the stack,
Sedgewick proposes sorting the smallest partition first. This optimization
reduces the maximum size of the stack to $O(logN)$. In practice, two \cc{int}s,
the left and the right index, are required for each partition.

Sedgewick's second improvement is the use of an elementary sort for small
subfiles.  Once a partition is smaller than an arbitrary threshold, insertion
sort is used to sort the partition. Sedgewick states that roughly the same
results are obtained for thresholds between 5 and 25. An additional reduction in
instruction count can be had by doing the insertion sort over the entire array
at the end, rather than across each small array as soon as the threshold is
reached. At the end of the partition sorting, LaMarca places a sentinel at the
end of the array and uses an insertion sort, eliminating the bounds check from
the end. In this implementation, the sentinel is placed at the front, and the
smallest of the first \cc{threshold} keys is used as a sentinel, in keeping
with our policy of not exceeding the bounds of the array.

Choosing a pivot which is close to the median of the keys to be sorted ensures
quicksort achieves optimal performance. Hoare suggested a random pivot, but the
added cost of a random number generator would severely slow the algorithm.
Sedgewick recommends that the median of three keys is chosen, the three keys
being the first, middle and last in the list. This reduces the chance of a worst
case occurring significantly, notably in the case of sorted and almost sorted
lists.

An additional benefit of Sedgewick's median of three implementation is that it
provides a sentinel for the partition. It is now possible to remove the bounds
check from the innermost loops of \cc{partition}, saving a lot of instructions.

Between these three improvements, Sedgewick estimates a reduction in
instruction count of roughly 30\%. The size of the auxiliary stack is also
reduced significantly, and the possibility of stack overflow, in the worst case
of the recursive version, is removed.

Quicksort has been well studied, and several important improvements have been
noted. In \cite{BentleyMcIlroy93}, using a median of nine partition is discussed,
and modern discussions on quicksort refer to the problem of equal keys, which is
discussed there, as well as by Sedgewick in \cite{SedgewickBentley02}. These
improvements are not included here for several reasons. Firstly, a quicksort
which takes into account equal keys puts the equal keys at the front of the
array, then copies them into place later. These copies would represent a lot of
cache misses, and the results would overwhelm the improvements LaMarca makes in
the next phase of the algorithm. Secondly, when sorting random keys over a
large range, the number of equal keys will be very low. Using a median of
greater than three is discussed in Section \ref{quick-predictors2}.

\section{Memory-Tuned Quicksort}

LaMarca's first step was to increase the temporal locality of the algorithm by
performing insertion sort as soon as the partitioning stops. Once a partition is
reduced below the size of the threshold, insertion sort is preformed
immediately. By doing this, the keys being sorted are in the cache already. By
leaving the insertion sort until the end, as Sedgewick does, the reduction in
instruction count is made up by the increase in cache misses, once the
lists being sorted are larger than the cache. 

As discussed above, it is necessary to insert either a bounds check or a
sentinel.  If the partition to be sorted is the left most partition, then the
smallest key in the partition will be chosen as the sentinel. No other
partition needs a sentinel. A possible way to avoid the sentinel in that
partition is to place zero at $0^{th}$ position in the list to be sorted, and
sort whichever number was in this position back at the end. This sort could be
done with one iteration of insertion sort, and would have just one branch miss.
However, the number of level 2 cache misses would be great in this case,
and the cost of this would be substantial. This could be reduced by
choosing the key to remove from a small list, such as the eight keys at the
start, middle and end of the array. This would introduce a constant number of
cache misses, but would significantly reduce the chances of having to sort
across the entire array at the end. This, however, is not implemented; instead a
check is done to fetch a sentinel from the appropriate subfile if required. This
check isn't very expensive, and is significantly cheaper than either a bounds
check or putting a sentinel into every subfile.

\section{Multi-Quicksort}

In the case that the array to be sorted fits in the cache, then quicksort has
excellent cache properties. Once it no longer fits in the cache, however, the
number of misses increases dramatically. To fix this problem, LaMarca uses a
technique similar to that used in multi-mergesort. Instead of multi-merging a
series of sorted lists at the end, a \n{multi-way partition} is done at the start,
moving all keys into cache sized containers which can be sorted individually.

At the start of the algorithm, a set of pivots is chosen and sorted, and keys
are put into containers based on which two pivots their value lies between. A
minimal and maximal key are put on the edges of this list to avoid bounds checks
as the list is iterated over. Because of the way the pivots were chosen, two
pivots which happen to be close together would result in a very small number of
keys being put into their container. While this is not a bad thing, this will
result in a larger number of keys in another container. Conversely, a large
space between two consecutive pivots will mean that the keys in this
container will not all fit into the cache at once.

There are two possible solutions to this. The first is to choose the pivots
well, so that they are very evenly distributed. The second is to choose the
number of pivots so that the average size of the containers is not larger than
the cache. LaMarca presents calculations showing that by choosing the average
subset size to be $C/3$, where $C$ is the number of keys which fit in the
cache, on average only 5\% of subsets will be larger than the cache.

Since the maximum number of keys which must fit in a container is not known, it
is necessary to implement containers using linked lists. Each list contains a
block of a few thousand keys. LaMarca notes very little performance difference
between 100 and 5,000 keys in each block. The size is therefore chosen so that
each list fits exactly into a page of memory. When a list is filled, another
list is linked to it, and these lists hold all the keys in the container.

The initial implementation of this allocated a new list every time an old one
was filled. This lowers performance significantly, as it results in a lot of
system calls, which increase instruction count and cache misses. Instead, it is
possible to work out the maximum possible number of lists required to create the
containers, and allocate them at the start. When a list runs out of space, it is
linked to an unused list from the large allocation. This wastes space, since it
is almost guaranteed that less lists will be required than available, though the
number of wasted lists allocated will be less than the number of pivots, and
therefore not exceptionally high.

The entire array is iterated though a key at a time. For each key, the list
of pivots are searched until the appropriate container is found, and the key is
added to that container. Sequentially searching the lists increases the
instruction count by 50\% over the memory-tuned version. For this reason, the
search was replaced with an efficient binary search, which reduced by half the
extra instructions.

Once the entire array has been put into containers, each container is emptied
in turn and the keys put back into the array. When a container is empty, the
pivot greater than that container is put into position, which is guaranteed to
be its final position.  The indices of these positions is pushed onto the
stack, and the subarray is then sorted.

While emptying the containers back into the array, an opportunity is taken to
find the smallest key in the leftmost partition. This is placed as a sentinel,
ensuring that every partition has a sentinel, and that no bounds checks are
required.

\section{Results}

\subsection{Test Parameters}

The threshold chosen was 10, simply as this is the number used in Sedgewick's
book.

The number of keys in the linked list is 1022 since a word is needed for the
count, and another as a pointer to the next list. 1024 is the number of 32-bit
words which fit in a 4KB page of memory. In earlier versions where each page was
\cc{malloc}ed individually, the number of keys in the linked list was 1018,
as 4 words were used by the particular version of \cc{malloc} used by
SimpleScalar\footnote{This particular \cc{malloc} forms part of
\cc{glibc}, the GNU C library, version 1.09.}.

A 2MB cache was assumed, so the number of 32 bit integer keys which fit in the
cache for this in-place sort is 524,288. Multi-quicksort began once the number
of keys was larger than 174,762.

\subsection{Expected Performance}

The instruction count for each quicksort is expected to be $O(NlogN)$, though
not as high as the other algorithms due to the efficient inner loop.  The number
of instructions for multi-quicksort should increase by about 50\% in the case of
the sequential sort, and 25\% in the case of the binary sort. Cache quicksort
should also have a slightly higher instruction count than base quicksort, due to
the change in use of insertion sort.

LaMarca notes that quicksort is very cache efficient already. It has excellent
spatial and temporal locality. Once it no longer fits in the cache, however, the
number of misses should increase significantly. This is especially true in the
case of the base quicksort, which does an insertion sort across then entire
array at the end.

Cache quicksort should have fewer misses than base quicksort as soon as the
array is larger than the cache. It is expected that these misses will make up
for the increase in instruction count.

LaMarca estimates a large reduction in level 2 misses in multi-quicksort, and
puts a figure on it of four misses per cache block, meaning, in our case, half a
miss per key.

The number of branches should increase in multi-quicksort along with the
instruction count. When a binary search is used, the increase in mispredictions
is expected to be significant. With a sequential search, however, the number of
misses should not increase at all, and should flat-line from that point on.

\subsection{Simulation Results}

\plot{quick}{}{}{Results shown are for a Direct Mapped cache.}{}{}{}{}

The instruction count, shown in Figure \ref{quick instructions} is moslty as
predicted. The increase in instructions in both multi-mergesorts is as
predicted, with the binary search version increasing only 25\% against the
sequential search version's 50\% increase. However, the increase we predicted in
the instrcution count of cache quicksort based on the results of both Sedgewick
and LaMarca -  that is, once the efficient insertion sort was replaced with many
small insertion sorts - we observe that the instruction count remains exactly
the same.


The cache results in Figure \ref{quick level 2 misses} were similar to
predictions, except that the multi-quicksorts did not behave as expected, and
performed as badly as the memory-tuned version. However, the rate at which the
cahce misses are increasing is very low, being almost a flat line on our graph.
LaMarca's calculation of four misses per cache block is very accurate, as we
show 0.55 misses per key, or 4.4 misses per block.

The tuned quicksort performed better than the base version, as predicted. Both the
tuned version and the base quicksort both have a significant increase once the
arrays no longer fit in the cache, also as predicted.

The results show again that direct mapped caches can perform better than fully
associative ones. This is true in the case of all the quicksorts, as can be seen
in Figures \ref{base quicksort (no median) level 2 misses} to
\ref{multi-quicksort (sequential search) level 2 misses} graph that this is true
in the case of all the quicksorts.

The branch prediction results are shown in Figure \ref{quick branch misses}. As
expected, there was a a dramatic increase in the binary search multi-quicksort
branch misses, but only when using a bimodal predictor. When using a 2-level
adaptive predictor, the number of misses is, in general, slightly higher, but it
does not spike due to the binary search. While it performs worse than the
bimodal predictor, it appears that 2-level adaptive predictors can adapt to the
erratic branches of a binary search.

The sequential search stops the increase of branch misses, since every key will
only take one miss to find its proper place. The results of this on cycle
count, shown in Figure \ref{quick cycles} are enough to make up for the increase
in instruction count, which was significantly reduced by using the binary
search.

Tuned quicksort had fewer branch misses than base quicksort, due to the fact
that fewer comparitive branches are executed. In base quicksort, we sort the
entire array using insertion sort. In tuned quicksort, we don't sort the pivots,
but only the keys between them.

Despite the sequentially search multi-quicksort performing better than its
binary searched cousin, and its good cache performance, it still failed to
approach the performance of either base or tuned quicksort. In fact, base
quicksort was the fastest of all the quicksorts in our performance counter test,
even though our simulator results show that it has more level 1 misses, level 2
misses, and branch misses than tuned quicksort, and the same number of
instructions.

LaMarca's results are again similar to the results presented here, but not
identical. The shape of the instruction count graph is the same in both cases,
but that of the cache misses is not. The ratio of the cache misses between base
and cache quicksort are the same in both sets of results, but these are lower
than the same misses in LaMarca's results. However, the multi-quicksort results
appear to be the same, though our results are not conclusive either way.


\section{Predictor Results}

%TODO a bit to rewrite
\subsection{Different Sorts}

\sixplot{base_quicksort_0.eps}{base_quicksort9_0.eps}{memory_tuned_quicksort_0.eps}{multi_quicksort_0.eps}{sequential_multi_quicksort_0.eps}{}{base, cache and multi quicksorts and base quicksort9}

Firstly, cache quicksort and base quicksort only differ in how they deal with
the insertion. Base quicksort and base quicksort9 have similar numbers of
insertion branches, and a very similar ratio (2/1 correct/incorrect) of
mispredictions. Comparing base quicksort to cahce quicksort (they both use
median of 3), there is actually a decrease in the number of branches handled by
insertion sort, instead of an increase. An increase is expected (thats why
sedgewick moves the insertion to the end), though when you take into account
the sentinel and the outer loop, there probably is an increase in instructions
at least. Base quicksort has 35\% miss rate, but cache has a 34\%. Not a big
difference, but the point is that there is a reduction in branches, but of the
reduced branches, 50\% were unpredictable. I dont think theres anything
interesting in this fact, however.

When going to multi-quicksort, there si quite a drop in the number fo branches.
Once the sort is larger than the cache, the number fo steps increases linearly
(since it does a sort of counting like radix sort). Theres a decrease of 19 \%.
Since the sort at this point is NlogM where M is the size of the cache (in
keys), there shouldnt be a change in misprediction rates. And there sint.
Before: 63\%. After, 63\%. So no difference.

Theres a lot of branches in the multi quicksort. WE saw in other results that
the mere instruction cost of this bogs down to algorithm significantly. in the
binary search version, there are more than twice as many mispredictions,
despite the fact that there are less than half the branches. (note that i dont
think this was a wonderful binary search, its heavily biased towards the
checking the left). binary search predicts correctly 57 percent of the time,
meaning that it mostly changes direction haphazardly during the search. No
wonder it performs so badly.

Note that sequential multi-quicksort has less mispredictions than cache quicksort.

%TODO to  here

\subsection{Different Medians}

\label{quick-predictors2}
\sixplot{base_quicksort1_0.eps}{base_quicksort_0.eps}{base_quicksort5_0.eps}{base_quicksort7_0.eps}{base_quicksort9_0.eps}{}{base quicksort without a median, with median of 3, and with pseudo medians of 5, 7 and 9}

\label{quicksort entropy}

In Section \ref{heapsort entropy}, we observed that when an algorithm has more
branches than are strictly necessary complete, these extra branches are
predicted correctly. In quicksort, we expect to obtain similar results.

%HOW

We use several variations of quicksort to test this theory. \ref{bently, i
ithkn} suggests partitioning using a median-of-9 instead of a median-of-three. We
created sorts using no median, a median-of-3, and pseudo-medians-of-five, -seven and
-nine. The pseudo-medians used repeated medians-of-three instead of a real
median, as suggestion by \ref{bently, again, i think}. 
%use a cut off

We measured the number of branches and mispredictions due to the partitioning,
the final insertion sort, and the partitioning. It is expected to have little or
no variance in either the number of branches and misses due to insertion sort,
but that less time will be spent partitioning to arrays as a better median is
found, and a corresponsing increase in time spent finding the better median.

The results of these tests are shown in Figure \ref{Branch prediction
performance for base quicksort without a median, with median of 3, and with
pseudo medians of 5, 7 and 9}. There is little variance in insertion sort misses
between the sorts using pseudo-medians, but the median-of-three quicksort has
6\% fewer branches, and a corresponsing decrease in mispredictions.
%TODO to whom do we owe the idea of a pseudo median? Bently/McIlroy?
%TODO when does it cut out, and why did we choose tere (benlty?)

Between a median-of-three and a pseudo-median-of-nine partitioning, there is a
10\% reduction in branches. At the same time, there is a 20\% reduction in
correct predictions, accounting for the entire reduction in branhces, and a 4\%
increase in mispredictions. This is due to better partitioning. Less time is
spent copying keys unnecessarily, resulting in a less predictable branch. The
corresponding increase in mispredictions while calculating the median equates to
50\% of the extra mispredictions due to partitioning.

This indicates the same pattern as we saw during heapsort.%
% and mergesort?
The less entropy due to the partitioning key, the less wasted exchanging of keys,
which leads to a general decrease in branches. This decrease means that the
fewer keys are exchanged due to partitioning, leading to less predictable
partitioning. In effect, there are fewer branches pretending to be useful.

The pseudo-median-of-five partitioning performs slightly better than the
median-of-three, and pseudo-median-of-seven performs slightly better again, in
terms of branch prediction. Figure \ref{TODO} shows the cycle count of each of
the quicksorts, showing that a pseduo-median-of-seven offers the greatest
tradeoff between the reduced number of branches and instructions, and the
increase in branch midpredictions. 



\section{Future Work}
The threshold in quicksort is the point at which a simpler sort is used to sort
the partition. A larger threshold will results in fewer checks for a sentinel. A
smaller threshold means that the instruction count due to the insertion sort
will be lower, but more partition steps may be taken. A smaller threshold also
makes it more likely that the keys to be sorted all fit in a single cache
block. However, the partition is likely to occur across a cache block boundary,
though this is not necessarily a bad thing; the next partition to be sorted will
be adjacent to the current block, and will already be in memory. Observing how
results, especially instruction count and branch misses, vary with changes 
to the threshold may be interesting.

Two areas of current research into quicksort are those of equal keys, and
median-of-greater-than-3. The former affects the algorithmic properties, and
some steps may need to be varied to avoid extra cache misses as a result
of extra copying. The latter simply affects the instruction count, though the
extra comparisons are likely to be mispredicted. These results could be
measured, giving greater insight to this debate.

The memory-tuned quicksort is the only algorithm here that requires a sentinel
check. The branch predictor will only mispredict this once, so the instruction
count increase is only that of removing a single check which occurs relatively
few ($N / THRESHOLD$)\footnote{This is a minimum. The figure may be higher since
partitions are likely to be smaller than the threshold.} times. Still, the
problem is interesting and has applications in other algorithm designs.

Finally, in multi-quicksort, it may be possible to choose better pivots.
Choosing better (ie, more evenly distributed pivots) would reduce the number of
containers. The advantage of this is that it reduces linearly the number of
pivots to be searched by the sequential search. The most this can be reduced by
is a factor of 3, indicating perfectly distributed pivots. This would reduce the
instruction overhead of the multi-quicksort considerably.
