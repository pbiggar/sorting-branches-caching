\label{merge}
Merging is a technique which takes two sorted lists, and combines them into a
single sorted list. Starting at the smallest key on both lists, it writes the
smaller key to a third list. It then compares the next smallest key on that list
with the next smallest on the other, and writes the smaller to the auxiliary
list. The merging continues until both lists are combined into a single sorted
list.

Mergesort works by treating an unsorted array of length $N$ as $N$ sorted arrays of
length one. It then merges the first two arrays into one array of length two, and
likewise for all other pairs. These arrays are then merged with their neighbours
into arrays of length four, then eight, and so on until the arrays are
completely sorted.

Mergesort is an out-of-place sort, and uses an auxiliary array of the same size
for the merging. Since a lot of steps are required, it is more efficient to
merge back and forth - first using the initial array as the source and the
auxiliary array as the target, then using the auxiliary array as the source and
the initial array as the target - rather than merging to the auxiliary array and
copying back before the next merge step.

The number of merges required is $log_2(N)$, and if this number is even then the
final merge puts the array in the correct place. Otherwise, the sorted array
will need to be copied back into the initial array.

\section{Base Mergesort}

\subsection{Algorithm N}
\label{Algorithm N}

For his base algorithm, LaMarca takes Knuth's algorithm N \cite{Knuth98}.
LaMarca describes this algorithm as functioning as described above; lists of
length one are merged into lists of length two, then four, eight, and so on.
However, algorithm N does not work in this fashion. Instead, it works as a
\n{natural} merge. Rather than each list having a fixed length at a certain
point, lists are merged according to the initial ordering. If it happens that
there exists an ascending list on one side, this is exploited by the algorithm.
While this results in good performance, the merge is not perfectly regular, and
it is difficult or impossible to perform several of the optimizations LaMarca
describes on it. We attempted to contact him in order to discuss this, but
received no response. It is likely that the improvements were actually applied
to Knuth's algorithm S.

\subsection{Algorithm S}
\label{Algorithm S}

Algorithm S is a \n{straight} merge, and works as described above. A
disadvantage of this method is that if an array has slightly more than $2^x$
keys, an entire extra merge step would be required, which would considerably
slow the algorithm. This problem is shared by all other versions of mergesort
considered here, except algorithm N. However, since our tests use exact powers
of two as set sizes, this problem does not affect them. LaMarca, however, used
set sizes such as 4096000, which at times would suffer from this type of
problem.

Algorithm S has 15\% lower instruction count than Algorithm N\footnote{This is
observed on random data, which algorithm N is not designed for. On more regular
data, or semi-sorted data, this will probably not occur.}. Algorithm S, like
algorithm N, is a very difficult algorithm to modify. Knuth describes it in
assembly, and making the optimizations that LaMarca describes on it is more
difficult than rewriting the algorithm more simply. The rewritten algorithm,
referred to from here on as `base mergesort', is a simple higher level version
of algorithm S, and should perform only slightly more slowly.


\subsection{Base Mergesort}
\label{base mergesort}
Three optimizations are described by LaMarca for the base mergesort. Firstly, a
bitonic sort, as described in \cite{Sedgewick02}, removes instructions from the
inner loop. Every second array is created in reverse, so that the larger keys  
are next to each other in the middle. The smallest keys are at the outside,
meaning that the lists are iterated through from the outside in. As a result, it
is not possible to go outside the array, and it is not necessary to consider the
lengths of the arrays, apart from when setting the initial array indices. When
the first array is exhausted, it will point to the last key on the other array,
which will be larger than all other keys in that array. When both are
exhausted, the last key of both arrays will be equal. A check is put in, in this
case, and merging ends if both indices are the same. Figure \vref{Visual Sort}
shows a bitonic merge.

The overhead of preparing arrays to be merged is small. However, for small
lists, the overhead is large enough that it can be more efficient to sort using
a much simpler sort, such as insertion, selection or bubblesort. In Section
\ref{insertion is predictable}, it is shown that insertion sort is the fastest
elementary sort. Sedgewick's quicksort uses this, and LaMarca recommends the use
of a simple inline pre-sort, which can be made from insertion sort. To make
lists of length four, the smallest of the four keys is found, and exchanged with
the first key, in order to act as a sentinel. The next three keys are sorted
with insertion sort. The next set of four keys is sorted in reverse using the
same method. It is possible to hard code this behaviour, in an attempt to reduce
the instruction count, but in fact the instruction count stays the same, and the
branch prediction suffers as a result.

Finally, the inner loop is \n{unrolled}\footnote{Unrolling a loop means
executing several steps of the loop between iterations and reducing the number
of iterations, which lowers the loop overhead and allows for better scheduling
of the instructions.}. There are actually two inner loops, and they are both
unrolled 8 times. The compiler is also set to unroll loops. However, the
observed difference between unrolling manually and compiler unrolling is up to
10\% of instruction count, so manual unrolling is used.

Overall, these three optimizations reduce the instruction count by 30\%.

\section{Tiled Mergesort}
\label{tiled mergesort}
LaMarca's first problem with mergesort is the placement of the arrays with
respect to each other. If the source and target arrays map to the same cache
block, the writing from one to the other will cause a conflict miss. The next
read will cause another conflict miss, and will continue in this manner. To
solve this problem, the two arrays are positioned so that they map to different
parts of the cache. To do this, an additional amount of memory the same size as
the cache is allocated. The address of
the auxiliary array is then offset such that the block index in the source array
is $CacheSize/2$ from the block index in the target array. The is achieved by
inverting the most significant bit of the block index in the target array.

While this step makes sense, whether this is valuable is very much a platform
dependant issue. Since virtual addresses are used, it may be possible that the
addresses used by the cache are not aligned as expected. Even worse, an
operating system with an excellent memory management system may do a similar
step itself. The program's attempt to reduce cache misses could then increase
them. In the event that the \n{Memory Management Unit} in the processor uses
virtual addresses to index its cache, or has a direct mapping between virtual
and physical or bus addresses, then this problem is very unlikely. The
simulations presented in this report address the cache by virtual address, and
so this optimization is effective in this case.

The next problem LaMarca considers is that of temporal locality. Once a key is
loaded into the array, it's only used once per merge step. If the data does not
fit in half the cache, then conflict misses will occur, and no temporal reuse
will occur at all. The solution is to apply a technique used in compilers called
\n{tiling}. Using this technique, an entire cache sized portion of the array
will be fully sorted before moving on to the next set. When all the sets have
been sorted in the manner, they are all merged together as usual.

It is also possible to avoiding the copy back to memory. In the event of an odd
number of merge steps, the auxiliary array will contain the sorted array, which
needs to be copied back. This can be avoided by pre-sorting to make lists sized
two or eight. A large increase of instruction count was realized by sorting to
lists of length two, so a length of eight was used. When an even number was
required, four was used.

\section{Double-aligned Tiled Mergesort}
\label{double-aligned tiled mergesort}
Offsetting the target array to avoid conflicts reduces significantly the number
of level 2 cache misses. However, it does this at the expense of increasing the
number of level 1 cache misses. To avoid this, we further offset the target
array by $Level1CacheSize/2$. It is expected that the number of level 1 misses
should be greatly decreased, at the expense of very few level 2 misses. The
expected improvement is not as great as between base mergesort and tiled
mergesort, but some improvement should be seen.

\section{Multi-mergesort}

Multi-mergesort has two phases. The first is the same as tiled mergesort, and
sorts the array into lists half the size of the cache. The second phase attempts
to improve temporal locality by merging these lists together in a single merge
step. There are $k$ lists and this is called a \n{k-way merge}. It is possible
to do a search across each of these keys, maintaining a list of indices to the
smallest unused key from each list. When there is a small number of lists, this
isn't a problem, however, when the number of lists increases significantly, the
overhead involved in this is substantial. It is necessary to check each of the
lists, and each key searched would cause a cache miss, since each array segment
is aligned, and map to the same cache block.

To avoid this, LaMarca uses a priority queue. The smallest key from each list is
added to the queue, and then put into the final target array. This reduces the
problem of the search overhead, but does not reduce the problem of
\n{thrashing}\footnote{Thrashing occurs when two arrays are mapped to the same
cache block, and the constant access of one followed by the other causes every
access to be a miss.}. To avoid this, an entire cache line is added at a time to
the priority queue. There will still be conflict misses, but there will be no
thrashing since the keys overwritten in the cache will not be needed again.

LaMarca used a 4-heap for his priority queue, and an 8-heap is used here, for
the same reasons as before (see Section \ref{memory-tuned heapsort}.). The heap
is aligned, and a cache line is put into the array each time. The smallest key
is at the top of the heap, and this is put into its final position in the array.
The heap is then fixed.

LaMarca uses a special tag to mark the last key from each list inserted into
the array. No more details are provided on this, and a sorted list of length $k$
is used here, in its place. If the key at the top of the heap is the same as the
smallest value in the list, another eight keys are added from the array segment
to which the number belonged. This continues until the array is full sorted.

Using a sorted list instead of LaMarca's tagging leads to a variety of problems.
The reason the sorted list is used is that no details of the tagging is
provided. The sorted list is $k$ keys long, and only needs to be properly sorted
once. The next time it needs to be sorted is when a new line is added, but in
this case only one key is out of place, and can be put in place with a single
insertion sort run. This is linear at worst, and will have only 1 branch
misprediction.

When a list is fully consumed, the next keys inserted will be from the next
list. This could be fixed with a bounds check, however, since the lists are
bitonic, the next keys inserted are going to be the largest keys from the next
array. These will be added to the heap, and take their place at the bottom,
without harmful effect. A harmful effect is possible, though, when lots of keys 
with the same value are in the array. All these keys would be inserted at a
similar time, and when the first one is taken off the heap, a list would add
eight keys, even though it had just added eight. In this manner, the size of the
heap could be overrun. With random keys over a large range, as in our tests,
this is unlikely, and so this is not addressed.

The heap used for the k-way merge should result in a large increase in
instructions, and a significant reduction in level 2 cache misses. The addition
of the heap should not result in an increase in misses, since the heap is small
and will only conflict with the merge arrays for a very short time. The heap is
aligned as described in the previous chapter, and has excellent cache
properties. LaMarca estimates that there should be one miss per cache line
in the first phase, and the same in the second phase, although each miss will
occur in both the source and target arrays. This sums to four misses per cache
block, for a total of one miss for every two keys in the array.

\section{Double-aligned Multi-mergesort}

Double-aligned Multi-mergesort combines the double alignment of Double-aligned
tiled mergesort with multi-mergesort.

\section{Results}
\subsection{Test Parameters}
The level 1 cache is 8K, so 1024 keys from each array can be sorted in the level
1 cache. 262144 keys can be sorted in the level 2 cache. When arrays were
pre-sorted, they were pre-sorted to a size four, or eight if the number of
passes was going to be odd. These were reversed for the double-aligned
multi-mergesort; since it does a k-way merge into the source array, the sorted
cache-sized segments need to be in the auxiliary array. An 8-heap was used for
the multi-mergesorts.

\subsection{Expected Performance}

Algorithm N is not optimized for random data, and should not perform as well as
algorithm S. Algorithm S is expected to have a lower instruction count than base
mergesort. Neither algorithm is expected to have good cache performance.


The instruction count of mergesort is $O(NlogN)$, and does not vary
according to input. Multi-mergesort should have a significantly higher
instruction count once its threshold is hit. The double merges have exactly the
same flow control as the single merges, so will only differ in cache misses.

The number of memory references is also $O(NlogN)$, and mergesort's cache
performance should be very regular, so long as the data set fits in the cache. Once
the data set exceeds the cache, the number of cache misses should leap. For
multi-mergesort, they should flatten out after this, perhaps with a slight
slope to take into account the conflict misses of the aligned array segments. The
fully-associative caches should not have these misses, and the results should be
correspondingly lower. The double mergesorts should have
better performance than their counterparts in level 1 misses, though there
should be a slight increase in level 2 misses. It is expected that they should
perform slightly faster as a result.

The flow control of mergesort is very regular, but there is no way to predict
the comparisons. As a result the misprediction rate should be $O(NlogN)$ for
all but the multi-mergesorts, which should increase once the threshold is
passed.

\subsection{Simulation Results}
\label{Mergesort results}

\plot{merge}{}{}{Results shown are for a direct-mapped cache.}{Results shown are
for a direct-mapped cache.}{}{}

As expected, algorithm S has lower instruction count than algorithm N, and 
also slightly outperforms base mergesort. The number of instructions, shown in
Figure \ref{merge instructions} is exactly as predicted, with a 50\% spike at
multi-mergesort. 

Level 2 misses, shown in Figure \ref{merge level 2 misses} are
also as expected, although the multi-mergesort does not show the same
improvement over tiled mergesort as in LaMarca's results, as our tiled mergesort
performs better. The cache performance of algorithms N and S are poor, as
expected. They perform roughly the same as base mergesort.

\label{mergesort replacement policy}
It is notable that our prediction of half a miss per key for multi-mergesort,
which was taken from LaMarca, is exactly right. However, our assertion that the
fully-associative cache will perform slightly better is false. In actual fact,
the direct-mapped cache performs significantly better (see Figure
\vref{multi-mergesort level 2 misses}). This is due to the replacement policy:
our fully-associative cache removes the least recently used keys from the cache
to make room, while the direct-mapped caches are designed to make best use of
the keys while they are in the cache. They have, effectively, an optimized
replacement policy.

The prediction results in Figure \ref{Branch simulation results for mergesort}
are as expected. Very little variation exists between the types of mergesort. It
is noticeable, though, that the spike in branch misses due to the
multi-mergesorts is considerably lessened by the two level adaptive predictor.
This shows there must be regular access patterns in the k-way merge at the end
of multi-mergesort, which are predictable using a branch history.

As predicted, the rate of level 1 misses is greatly reduced by the double
tiling, as shown in Figure \ref{merge level 1 misses}. Double-aligned multi-mergesort
has consistently four times fewer level 1 misses than multi-mergesort, and both
double tiled sorts perform significantly better than the unoptimized sorts. The
expected increase in level 2 misses did not materialise, and is in fact
negligible.

The level 2 cache results are startlingly similar to LaMarca's. The shape of the
Figure \ref{merge level 2 misses} is exactly that provided by LaMarca. The
ratio's between the each of the sorts is also the same, as each of our results
is exactly half that of LaMarca, due to different \cc{int} sizes in our tests.

Despite the improvement in cache misses due to multi-mergesort, it still cannot
compensate for the increase in instruction count as a result, and it has a higher
cycle count on the Pentium 4 (see Figure \ref{merge cycles}) as a result. The
other sorts generally perform in $O(NlogN)$ time, with double-aligned tiled mergesort
being the fastest. Note that the alignment of the mergesort may not work as
desired due to the size of the cache. This was optimized for a 2MB cache, while
the Pentium 4 cache is only 256KB. This means that the two arrays are aligned to
the same cache block. However, since the cache is 8-way associative, this is not
serious, and should not result in a lot of thrashing.

The cache misses end up being the deciding factor between the performance of
double-aligned tiled mergesort and base mergesort, the next best contender.
Despite the reduction in level 2 cache misses, the sharp increase in level 1
misses result is poor performance for tiled and multi-mergesort. Multi-mergesort
is further encumbered with a much higher instruction count. Double-aligned tiled
mergesort, with its low instruction count, and low cache miss rate in both
levels 1 and 2, emerges the winner.

\section{A More Detailed Look at Branch Prediction}

\sixplot{algorithm_n_0.eps}{algorithm_s_0.eps}{base_mergesort_0.eps}{double_aligned_tiled_mergesort_0.eps}{double_aligned_multi_mergesort_0.eps}{double_aligned_multi_mergesort_1.eps}{mergesorts}

Figure \ref{mergesorts-1} shows the behaviour of the individual branches within
algorithm N. The labels in the graphs correspond to the names of the steps in
Knuth's algorithm (we also used the same names for the labels of branch targets
in our C source code. Branch N3 is the main comparison branch; it is used to
choose the smaller of the elements from the start of the two sub-arrays being
merged. The direction of this branch is completely random - it depends entirely
on the (in this case random) data. Figure \ref{mergesorts-1} also shows that
this branch is almost entirely unpredictable using a bimodal predictor. The
result for a two-level predictor is much the same, because there is no pattern
in the outcome of these branches. Branches `\n{N3 i==j}' (check for the current
merge being complete), `\n{N5}' and `\n{N9} (both checks for the end of a run of
increasing elements) are control-flow branches, and are almost entirely
predictable. Figure \ref{mergesorts-2} shows that the comparison branch in
Algorithm S is just as unpredictable as that in algorithm N. However, given that
the merge in algorithm S is entirely regular (see section \ref{Algorithm S}) it
is possible to eliminate one of the control-flow branches.

Base mergesort is an optimized algorithm that reverses the sorting direction of
every second sub-array, pre-sorts small sub-arrays, and applies a number of
other optimizations. As Figure \ref{mergesorts-3} shows, our base mergesort has
eliminated some of the control-flow branches. The main comparison branch is
split into two separate branches - one iterating from left-to-right over the
increasing sub-array (`\n{Left}'), and the other from right-to-left over the
decreasing sub-array (`\n{Right}'). However, base mergesort appears to need a
total of 50\% more comparison branches that either Algorithm S or N.

In fact, base mergesort eliminated control-flow branches by using the same
branch that is used for comparison to also deal with the case where one (or
both) of the merging sub-arrays becomes empty. Thus, the sum of the number of
executions of the branches `\n{Left}' and `\n{Right}' is almost exactly the same
as the sum of the number of executions of the branches N3 and N3 i==j in
Algorithm N. The total number of mispredictions is also almost the same.

The comparison branch in double-aligned tiled mergesort is divided into four
separate branches. As in base mergesort, there are `\n{Left}' and `\n{Right}'
branches. However, for each of these there are separate branches for when
creating a new sub-array in increasing order (`\n{Forwards}') or decreasing
order (`\n{Reverse}'). There are slightly more executions of `\n{Forwards}' than
`\n{Reverse}' because the final array is always sorted into increasing order.
The left merge step is also favoured, but this is merely due to the fact that it
occurs first in the code. While there are more left branches, the actual number
of branches taken are the same.

Double aligned multi-mergesort is an even more complicated algorithm. It sorts
the arrays fully in the cache, leaving them bitonically arranged (sorted into
sub-arrays, sorted into alternately increasing and decreasing order). The final
merge is performed separately, and hence no bias for the forward branches, as
there was in tiled mergesort. The final sort is performed with a k-way merge,
which is implemented using a heap data structure. This is shown in Figures
\ref{mergesorts-5} and \ref{mergesorts-6}. The `\n{Comparison}' branches are
comparisons from the merge. The labels in these graphs are the same as in Figure
\ref{mergesorts-4} and \ref{base and memory-tuned heapsorts-2}.

During regular two-way merging, the number of branches is 20\% lower than base
mergesort, while the number of mispredictions in only 10\% lower. This implies
that most of the mispredictions occur early in the merging process, and not in
the final merge, which multi-mergesort leaves out.

Overall, when the $k$-ary heap is used, there are approximately the same total
number of branch mispredictions as in the other mergesort algorithms. Although
the final k-way merge executes a lot more comparison branches, these heap
branches are quite predictable, as we saw in Section
\ref{heapsort-branch-prediction}. This means that using the technique to reduce
cache misses does not adversely affect branch prediction rates (it does,
however, still affect the instruction count). The rate of branch misses is much
the same as that in heapsort, and progresses in the same manner (see Figure
\ref{memory-tuned heapsort curve}). Note that `\n{comparison 0}' is included for
illustration purposes only. It is a comparison between two values, one of which
has just been assigned to the other, and which are therefore equal. It is
guaranteed to be taken every time, and would be optimized out by the compiler. 


\section{Future Work}
Several avenues could be pursued in order to improve the algorithms presented
here. In multi-mergesort, LaMarca spoke of tagging keys as they went into the
cache. Here, a sorted list was used in its place. Implementing and investigating
tagging and comparing of the results of the two techniques would be interesting.

Algorithm N is a mergesort for more natural layouts of data, such as partially
sorted lists. It can sort these faster than a straight merge, but won't on
random data until it is nearly sorted. Some of the improvements described here
may be able to improve its performance, as they did for the straight merge.

To address one of the problems with multi-mergesort, that of the conflict misses
on the k-way merge, \cite{Xiao00} reveals a padded multi-mergesort with reduced 
conflict misses. It would be interesting to compare these results.

It may also be possible to simply reduce the number of conflict misses of the
k-way merge, simply by reducing the size of the initial sort by the size of
several cache lines. In this way, the smallest keys would map to different
cache blocks, without the need for padding.
