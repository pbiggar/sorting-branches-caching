\label{method}
This section discusses the method and framework used to complete this research.
The first section discusses the steps involved in creating a testing framework,
programming the sorts and getting results. The second section discusses tools
that were used or created to do this.

\section{Method}

The following were the objectives of the research:
\begin{itemize}
\item Implement heapsort, mergesort, quicksort, radixsort, shellsort and the four
elementary sorts: insertion sort, selection sort, bubblesort and shakersort.
\item Implement cache-conscious sorts from \cite{LaMarca96} and
\cite{LaMarca99}.
\item Simulate these sorts using the SimpleScalar architecture simulator, using a
variety of branch predictors and cache configurations.
\item Measure the performance of these sorts using the PapiEx performance
counters.
\item Simulate bimodal branch predictors in software and use them to measure the
sorts.
\item Investigate and analyse the increase in speed, reduction in cache misses, 
and change in branch prediction accuracy as a result of the changes made.
\item Determine if any changes can be made to improved the sorts' performance.
\end{itemize}

Several steps were necessary to prepare for the research. A framework was
required by which sorts could be tested as they were being developed. This is
discussed in Section \ref{Main}. It was also necessary to fully understand the
technologies to be used in the report. Several texts were used for this: cache
and branch predictors are both in \cite{HennessyPatterson90}; branch predictors
in \cite{Uht97}, as well as in \cite{McFarling93} and \cite{Smith81}; sorts are
discussed in \cite{Sedgewick02} and \cite{Knuth98}; cache-conscious sorts are
discussed \cite{LaMarca96}, \cite{LaMarca99} and \cite{LaMarcaHeap96}.

There were many steps involved in writing each iteration of the algorithms.
Insertion and selection sort had just one of these iterations; the other
sorts had more than one\footnote{Quicksort, for example, had four: base
quicksort, tuned quicksort, multi-quicksort and sequential multi-quicksort.}. A
flowchart of these steps appears in Figure \vref{Flowchart}.

\begin{figure}
\scalebox{0.57}{\includegraphics*[0pt,165pt][600pt,802pt]{flowchart}}
\caption{Flowchart describing the development of an algorithm}
\label{Flowchart}
\end{figure}


The first step in each case was to write the algorithm. \cite{Sedgewick02} was
the primary source of these, though in several cases LaMarca specified a
particular version which he used. When a step involved in an algorithm wasn't
clear, the paper describing the algorithm was used to clarify the meaning.

Once the algorithm was written, a testing framework was used. This tested for
edge-cases in the algorithms by testing sorts sized from \cc{8} to \cc{255}
keys, with different random data sets. Testing was also performed on a very
large data set. This test was timed and the results were considered. The
results of this test were not accurate: the program took up to a minute to run,
during which time the computer was being used by multiple users and processes,
leading to context switches, sharing of resources and scheduling. Despite these
inaccuracies, it was possible to use the results relative to each other, and to
determine whether, for example, memory-tuned quicksort was running faster than
base quicksort.

Early on, it was decided that exceeding the bounds of an array
during a sort was to be forbidden. No serious sorting library or application
could acceptably work in this manner, and any results obtained while allowing
this were only of academic interest. It isn't possible, from a normal run of a
program, to determine if an arrays bounds are exceeded slightly, though exceeding
the bounds by a large amount will result is a segmentation fault. Because of
this, Valgrind (see Section \ref{Valgrind}), was used to report these errors.

Once it was certain that a sort actually sorted without error, it was necessary
to ensure that this was not a fluke occurrence. Many of the sorts had multiple
passes, and if one pass did not properly sort the array segment it was meant to,
the array might still be sorted by the next pass, though potentially much more
slowly. To test that this did not occur, a set of functions to visually display
arrays and variables at certain points was developed, which created a HTML page
showing the progression of the algorithm. An example of this is in Figure
\vref{Visual Sort}.

\begin{figure}
\scalebox{0.58}{\includegraphics*{visual}}
\caption{A sample visual sort routine, showing a merge from the source array to
the auxiliary array}
\label{Visual Sort}
\end{figure}

This shows two stages of a bitonic mergesort which is described in Section
\ref{base mergesort}. The top of the image shows the source array being merged
into the target array below. The source array shows three large segments of the
array and four smaller segments. The first two large segments have been merged
into one segment in the target array. The third of these has been merged into
the corresponding target array segment. The first of the smaller segments has
begun being merged with the other small segments. The rest of the target array
is uninitialised. Below that is a small amount of debugging information. $i ==
j$ is a key point in the program and is printed as debugging information. In the
two arrays below this, the merge continues.

While it was not known in advance the desired instruction count of an algorithm,
a certain range was implied by LaMarca's descriptions. The relationship between
one iteration of an algorithm, or between base versions of several algorithms
was known ahead of time. One configuration of the \cc{simple} script ran quick
tests using the \cc{sim-fast} simulator, giving instruction counts which could
be compared to other versions. 

If both these tests and the visual display of the sort's behaviour seemed
correct, then the full collection of simulations was run. The results of the
simulation were compared to results of other versions and to the expected
results. If the results were surprising, then it was necessary to explain the
reasons for this. If the cause was a bug, then the bug was fixed and the tests
repeated.

The results were then turned into graphs, from which conclusions about the
performance of the algorithm could be reached, and patterns could be spotted.
More details of this are in Section \ref{Gnuplot}. The next iteration of the
algorithm was then begun, and the process repeated for each of the algorithm's
iterations.

Finally, a software predictor was added in order to pin-point the cause of the
branch mispredictions. This meant altering the code of the algorithms to call
functions which updated simulated branch predictors. Each important branch was
allocated a predictor, which was updated independently of the other predictors.
This allowed the performance to be analysed down to the individual branch, and
blame apportioned appropriately.

\section{Tools}

\subsection{SimpleScalar} \n{SimpleScalar} is an architecture simulator. It
works by compiling code into its native instruction set, \n{PISA}, then
emulating a processor as it runs the program. Documentation on how to use and
modify SimpleScalar are \cite{Burger97}, \cite{Austin02}, \cite{Tutorialv4},
\cite{Tutorialv2} and \cite{SimpleScalarUserGuide}. SimpleScalar can be found at
\n{http://www.simplescalar.com}.

SimpleScalar works by compiling code into its own instruction set. It provides a
compiler - a port of gcc - with related tools such as a linker and assembler.
Several simulators are then provided: \cc{sim-fast} only counts instructions
executed; \cc{sim-cache} simulates a cache and counts level 1 and level 2 cache
misses; \cc{sim-bpred} simulates a branch predictor and counts branch hits and
misses. Other simulators are provided, but they were not used in our tests.

The output of SimpleScalar is human readable, but is not conducive to collecting
large amounts of data. To that end, a script was written to run each required
set of simulations. It recorded data from the simulations into a single file for
later retrieval. This file was suitable to reading with a \cc{diff} program.

\subsection{Gnuplot}
\label{Gnuplot}
\n{Gnuplot} is a tool for plotting data files. It provides visualisations of
data in two and three dimensions, and is used to create all of the graphs in
this document. Its homepage is \n{http://www.gnuplot.info/}.

There were several steps involved in using gnuplot for the data. Firstly, the
data was not in the correct format for gnuplot, and had to be converted. It also
needed to be offset by the cost of filling the array with random variables, and
scaled to so that the y-axis could be in the form of \n{plotted data per key},
both to be more readable, and to be consistent with LaMarca's graphs. Once the
data was converted, it was necessary to generate a gnuplot script for each
graph. The number of graphs required made this an impossible manual task, and
so another script had to be written. Some graphs still needed to be tweaked by
hand, due to difficulties in the script, but the task was much shortened by
this, and the tweaking was mostly accomplished using macros in the development
platform.

Finally, a script to compare the data together was required. This compared each
sort against each other sort in key metrics, such a branch or cache misses. It
also compared each cache size against other cache sizes for a particular sort,
and branch predictors against other branch predictors for each sort. 

\subsection{Main}
\label{Main}

The main program provided the framework for the development of the sorts.
Imaginatively called \cc{main.c}, this program provided the means for testing
and (roughly) timing each sort. Figure \vref{Key sections of main.c} contains a
sample of this code.

\code{Key sections of main.c}{main.c}

\subsection{Valgrind}
\label{Valgrind}
\n{Valgrind} is a set of tools which are used for debugging and profiling. By
default it works as a memory checker, and detects errors such as the use of
uninitialised memory and accessing memory off the end of an array. This use
makes it particularly useful for bounds checking. No configuration was required
for this, and simply running \cc{valgrind a.out} ran the program through
valgrind. The homepage for valgrind is at \n{http://valgrind.kde.org/}.

\subsection{PapiEx}
\n{PapiEx} is a tool used to access performance counters on a Pentium 4. It uses
the \n{PAPI} framework to count certain characteristics across a program's
execution. We used it to count the number of cycles a sort took to run. This
provided an accurate measure of how fast a sort executes on a Pentium 4. The
PapiEx homepage is at \n{http://icl.cs.utk.edu/\textasciitilde{}mucci/papiex/}.

PapiEx was used to create the graphs comparing the running time of sorts.
The gnuplot scripts for this were short and easily written by hand.

PapiEx was an alternative to using SimpleScalar. By using its hardware
registers, actual real world results could have been measured. Using PapiEx
instead of SimpleScalar was an option reviewed at the start, but because PapiEx
measures results on a real machine, only one type of cache and branch predictor
could have been measured. We have instead provided limited measurements to
supplement the SimpleScalar results.

\subsection{Software Predictor}
The software predictor used was a series of macros and functions which
maintained the state of a bimodal branch predictor at a particular point in the
program. Statistics about each branch were collected, and dumped into text
files, from which scripts created graphs to display them. Some sample code is
shown in Figure \ref{Sample code using the software predictor}. Each of the
graphs produced by the predictors uses the same scale; this way, it is possible
to visually compare the results from one sort to another.

\code{Sample code using the software predictor}{quick.c}

\section{Sort Implementation}

To show an example of the creation of a sort, mergesort will now be discussed.
Descriptions of some of the terms here is saved for discussion in Chapter
\ref{merge}.

The base mergesort described by LaMarca is Algorithm N. Three improvements
were prescribed by LaMarca: changing the sort to be bitonic to eliminate bounds
checks, unrolling the inner loop, and pre-sorting the array with a simple in-place
sort. He also recommended making the sort pass between two arrays to avoid
unnecessary copying.

Algorithm N is described in \cite{Knuth98}. A flow control diagram and step by
step guide is provided, but very little detail of how the sort actually works is
included. Therefore, the first step was to understand how this sort worked. This
was done by first analysing the flow control of the algorithm. The steps of the
sort are described in assembly, and while they can be written in C, the result
is a very low-level version. An attempt was made to rewrite the algorithm
without \cc{goto} statements and labels, instead using standard flow control
statements such as \cc{if}, \cc{while} and \cc{do-while}. Despite the time spent
on this, it was impossible to faithfully reproduce the behaviour without
duplicating code.

As a result, it was necessary to use the original version, and to try to perform
optimizations on it. However, several factors made this difficult or impossible.
LaMarca unrolled the inner loop of mergesort, but it is not clear which is the
inner loop. The two deepest loops were executed very infrequently, and
unrolling them had no effect on the instruction count. Unrolling the other loops
also had no effect, as it was rare for the loop to execute more than five or six
times, and once or twice was more usual.

Pre-sorting the array was also not exceptionally useful. The instruction count
actually increased slightly as a result of this, indicating that algorithm N
does a better job of pre-sorting the array than the inlined pre-sort did.

Algorithm N does not have a bounds check. It treats the entire array
bitonically; that is, rather than have pairs of array beings merged together,
the entire array is slowly merged inwards, with the right hand side being sorted
in descending order and the left hand side being sorted in ascending order.
Since there is no bounds check, there is no bounds check to be eliminated.

As a result of this problems, algorithm S, from the same source, was
investigated. This behaved far more like LaMarca's description: LaMarca
described his base mergesort as sorting into lists of size two, then four and
so on. Algorithm S behaves in this manner, though Algorithm N does not. Fully
investigating algorithm S, which is described in the same manner as algorithm N,
was deemed to be too time-consuming, despite its good performance.

As a result, a mergesort was written partially based on the outline of algorithm
N. The outer loop was very similar, but the number of merges required were
calculated in advance, and the indices of each merge step. It was very simple
to turn this into a bitonic array; adding pre-sorting reduced the instruction
count, as did unrolling the loop. The reduction in instruction count from the
initial version, which performed the same number of instructions as the same as
algorithm N, and the optimized version, was about 30\%.

A tiled mergesort was then written, based on LaMarca's guidance. Firstly, this
was aligned in memory so that the number of misses due to conflicts would be
reduced. Then it fully sorted segments of the array, each half the size of the
cache, so that temporal locality would be exploited. It also fully sorted
segments within the level 1 cache. Finally, all these segments were merged
together in the remaining merge steps.

LaMarca's final step was to create a multi-mergesort. This attempted to reduce
the number of cache misses in the final merge steps of tiled mergesort by
merging all the sorted arrays in one step. This required the use of an $k$-ary
heap, which had been developed as part of heapsort.

Once these sorts were written it was attempted to improve them slightly.
Instead of a tiled mergesort, a double-aligned tiled mergesort was written.
This aligned the arrays to avoid conflicts in the level 1 cache, at a slight
cost to level 2 cache performance. Double-aligned multi-mergesort was a
combination of this and of multi-mergesort. We also rewrote base mergesort far
more cleanly, with the results being more readable and efficient. These were
also found to reduce the instruction count, as well as the level 2 cache miss
count. These results are discussed in more detail in Section \ref{Mergesort
results}.

Each of these sort variations had to be fully written from the descriptions in
LaMarca's thesis. Only the base algorithm was available from textbooks, and this
needed to be extensively analysed and rewritten. Each step which was written had
to be debugged and simulated. The results were compared with the expected
results and frequently the sorts needed to be edited to reduce the cost of a
feature.

Radixsort, shellsort and each of the four $O(NlogN)$ sorts were treated in this
fashion.

\section{Testing Details}

Our tests were designed so that we could compare our results to LaMarca's, and
so that our results would hold in the general case. Both our testing method and
our data sets were chosen with these goals in mind.

\subsection{SimpleScalar Simulations}

We tested each of our sorts using SimpleScalar, using set sizes from 4096 to
4194304, increasing by a factor of two each time. Each of these set sizes was
simulated using \cc{sim-cache} and \cc{sim-bpred} in a variety of
configurations.

The SimpleScalar settings were chosen to be similar to LaMarca's simulations.
LaMarca used the \n{Atom} simulator, which was used to test software for the DEC
AlphaStation. It simulated a 2MB direct-mapped cache with a 32-byte cache line.
Other settings were also chosen to correspond to the DEC Alpha: 8KB level 1 data
cache, 8KB level 1 instruction cache and shared instruction and data level 2
cache.

In order to be able to extrapolate more information, we increased the number of
cache simulations. We simulated both 2MB and 4MB caches, configured as both
direct-mapped and fully-associative.

We chose a similar approach for our branch predictor caches. We aimed to
simulate bimodal predictors as well as two-level adaptive, in order to be able to
compare them together. We simulated bimodal predictors with a 512, 1024 and
2048 byte table, and two-level adaptive predictors with 512, 1024, 2048 and 4096
entries.

\subsection{Software Predictor}
In order to explore the behaviour of the individual branches, we performed a
second set of branch prediction experiments. Rather than using SimpleScalar, we
instead added additional statements to the source code to simulate a separate
bimodal predictor for each interesting branch. This allowed us to measure each
branch individually, whereas SimpleScalar gives only a total figure for all
branches.

The software predictor results are averaged over ten runs, using arrays
containing 4194304 keys. To compare these results to the SimpleScalar ones, it
is necessary to divide the number of branch mispredictions by 4194304.

\subsection{PaPiex}
Our papiex tests ran significantly faster than either of our other simulations,
and so we were able to run them over a significantly greater quantity of data.
Each sort and each size from 4096 to 4194304 keys was run 1024 times, using the
system's random number generator to generate data, using seeds of between 0 and
1023. The results displayed are averaged over the 1024 runs.

These simulations were run a on Pentium 4 1.6GHz, which had an 8-way, 256K level
2 cache, with a 64-byte cache line. The level 1 caches are 8K 4-way associative
caches, with separate instruction and data level 1 caches.

The instruction pipeline of the Pentium 4 is 20 stages long, and so the branch
misprediction penalty of the Pentium 4 approaches 20 cycles. The Pentium 4's
static branch prediction strategy is backward taken and forward not taken; its
dynamic strategy is unspecified: though \cite{Intel248966-010} mentions a branch
history so it is likely to be a two-level adaptive predictor.

\subsection{Data}

Both our SimpleScalar simulations and our software predictor tests used
\n{http://www.random.org} to provide truly random data. The software predictor
used ten sections containing 4194304 \n{unsigned integer}s, and the SimpleScalar
simulations used the same ten sections. When SimpleScalar simulations were run
on sized smaller than 4194304 keys, only the left hand side of the sections were
used.

\subsection{32-bit Integers}
LaMarca used 64-bit integers, as his test machine was a 64-bit system.
Since our machine is 32-bit, and SimpleScalar uses 32-bit addressing on our
architecture, LaMarca's results will not be identical to ours. Rather, the
number of cache misses in his results should be halved to be compared to ours;
when this is not the case, it is indicated.

\subsection{Filling Arrays}

It takes time to fill arrays, and this can distort the relationship between the
results for small set sizes, and larger set sizes, for which this time is
amortised. As a result, we measured this time, and subtracted it from our
results.

However, this practice also distorts the results. It removes compulsory cache
misses from the results, so long as the data fits in the cache. When the data
does not fit in the cache, the keys from the start of the array are faulted,
with the effect that capacity misses reoccur, which are not discounted. As a
result, on the left hand side of cache graphs, a number of compulsory misses are
removed, though they are not on the right hand side. LaMarca does not do this,
and so this needs to be taken into account when comparing his results to ours.


\section{Future Work}
An improvement that would have yielded interesting results would be to do
simulations which only consisted of flow control, with no comparisons. This
would allow a division of flow control misses and comparative misses. However,
this would not always be possible; sorts like quicksort and heapsort use their
data to control the flow of the program. In addition, bubblesort, shakersort and
selection sort have (analytically) predictable flow control, while radixsort has
no comparison misses at all. Mergesort may benefit from this, however,
especially versions with difficult flow control diagrams, such as \n{algorithm
N} and \n{algorithm S} (see Section \ref{Algorithm N}).

A metric not considered so far is the number of instructions between branches.
This should be an important metric, and investigating it could point to
important properties within an algorithm.

Valgrind comes with a tool called \cc{cachegrind}, which is a cache
profiler, detailing where cache misses occur in a program. Using this on a
sort may indicate areas for potential improvement.
