%\section{Description}

Quicksort is an in-place, unstable sort. It was first described in
\cite{HoarePartition61} and \cite{HoareQuicksort61}, and later in more detail in
\cite{Hoare62}. Sedgewick did his PhD on quicksort, and several important
improvements to quicksort are due to this. They are discussed in
\cite{Sedgewick78} and \cite{Sedgewick02}. \cite{BentleyMcIlroy93} discusses
considerations for a fast C implementation. \cite{SedgewickBentley02} discusses
equal keys and proves that quicksort has the lowest complexity of any comparison
based sort.

Quicksort is an algorithm of the type known as \n{divide and conquer}. It
recursively partitions its array around a pivot, swapping large keys on
the left hand side with smaller keys on the right hand side. The partition
causes the pivot to be put in its final position, with all greater keys to the
right, and lesser keys on the left. The left and right partitions are then
sorted recursively.

The recursive nature of quicksort means that even though quicksort is in-place,
there is a requirement of a stack. In the worst case, this stack can grow to
$O(N)$, if the largest or smallest key is always chosen as the pivot. This
worst case will occur frequently, especially when trying to sort an already
sorted, or almost sorted list.

Figure \vref{Simple quicksort implementation} contains a simple implementation of
the quicksort algorithm.

\code{Simple quicksort implementation}{simple_quicksort.c}

\section{Base Quicksort}

LaMarca uses the optimised quicksort described by Sedgewick for his base
quicksort. This is an iterative quicksort which uses an explicit stack. In the
worst case, this stack must be the same size as the array, and must be
initialised at the start of the algorithm. To reduce the size of the stack,
Sedgewick proposes sorting the smallest partition first. This optimisation
reduces the maximum size of the stack to $O(logN)$. In practice, two \cc{int}s,
the left and the right index, are required for each partition.

Sedgewick's second improvement is the use of an elementary sort for small
subfiles.  Once a partition is smaller than an arbitrary threshold, insertion
sort is used to sort the partition. Sedgewick states that roughly the same
results are obtained for thresholds between 5 and 25. An additional reduction in
instruction count can be had by doing the insertion sort over the entire array
at the end, rather than across each small array as soon as the threshold is
reached. At the end of the partition sorting, LaMarca places a sentinel at the
end of the array and uses an insertion sort, eliminating the bounds check from
the end. In this implementation, the sentinel is placed at the front, and the
smallest of the first \cc{threshold} keys is used as a sentinel, in keeping
with the policy of not exceeding the bounds of the array.

Choosing a pivot which is close to the median of the keys to be sorted ensures
quicksort achieves optimal performance. Hoare suggested a random pivot, but the
added cost of a random number generator would severely slow the algorithm.
Sedgewick recommends that the median of three keys is chosen, the three keys
being the first, middle and last in the list. This reduces the chance of a worst
case occurring significantly, notably in the case of sorted and almost sorted
lists.

An additional benefit of Sedgewick's median of three implementation is that it
provides a sentinel for the partition. It is now possible to remove the bounds
check from the innermost loops of \cc{partition}, saving a lot of instructions.

Between these three improvements, Sedgewick estimates a reduction in
instruction count of roughly 30\%. The size of the auxiliary stack is also
reduced significantly, and the possibility of stack overflow, in the worst case
of the recursive version, is removed.

Quicksort has been well studied, and several important improvements have been
noted. In \cite{BentleyMcIlroy93}, using a median of nine partition is discussed,
and modern discussions on quicksort refer to the problem of equal keys, which is
discussed there, as well as by Sedgewick in \cite{SedgewickBentley02}. These
improvements are not included here for several reasons. Firstly, a quicksort
which takes into account equal keys puts the equal keys at the front of the
array, then copies them into place later. These copies would represent a lot of
cache misses, and the results would overwhelm the improvements LaMarca makes in
the next phase of the algorithm. Secondly, it is not as important to write a
brilliant version of quicksort as it is to analyse LaMarca's improvements.

\section{Memory-Tuned Quicksort}
LaMarca's first step was to increase the temporal locality of the algorithm by
performing insertion sort as soon as the partitioning stops. Once a partition is
reduced below the size of the threshold, insertion sort is preformed
immediately. By doing this, the keys being sorted are in the cache already. By
leaving the insertion sort until the end, as Sedgewick does, the reduction in
instruction count is made up by the increase in cache misses, once the
lists being sorted are larger than the cache. 

As discussed above, it is necessary to insert either a bounds check or a
sentinel.  If the partition to be sorted is the left most partition, then the
smallest key in the partition will be chosen as the sentinel. No other
partition needs a sentinel. A possible way to avoid the sentinel in that
partition is to place zero at $0^{th}$ position in the list to be sorted, and
sort whichever number was in this position back at the end. This sort could be
done with one iteration of insertion sort, and would have just one branch miss.
However, the number of level 2 cache misses would be great in this case.
However, the cost of this would be substantial.  This could be reduced by
choosing the key to remove from a small list, such as the eight keys at the
start, middle and end of the array. This would introduce a constant number of
cache misses, but would significantly reduce the chances of having to sort
across the entire array at the end. This, however, is not implemented; instead a
check is done to fetch a sentinel from the appropriate subfile if required. This
check is expensive, but less so than either a bounds check or putting a sentinel
into every subfile.

\section{Multi-Quicksort}

In the case that the array to be sorted fits in the cache, then quicksort has
excellent cache properties. Once it no longer fits in the cache, however, the
number of misses increases dramatically. To fix this problem, LaMarca uses a
technique similar to that used in multi-mergesort. Instead of multi-merging a
series of sorted lists at the end, a \n{multi-way partition} is done at the start,
moving all keys into cache sized containers which can be sorted individually.

At the start of the algorithm, a set of pivots is chosen and sorted, and keys
are put into containers based on which two pivots their value lies between. A
minimal and maximal key are put on the edges of this list to avoid bounds checks
as the list is iterated over. Because of the way the pivots were chosen, two
pivots which happen to be close together would result in a very small number of
keys being put into their container. While this is not a bad thing, this will
result in a larger number of keys in another container. Conversely, a large
space between two consecutive pivots will mean that the keys in this
container will not all fit into the cache at once.

There are two possible solutions to this. The first is to choose the pivots
well, so that they are very evenly distributed. The second is to choose the
number of pivots so that the average size of the containers is not larger than
the cache. LaMarca presents calculations showing that by choosing the average
subset size to be $C/3$, where $C$ is the number of keys which fit in the
cache\footnote{For a 2MB cache, this is 524,144.}.

Since the maximum number of keys which must fit in a container is not known, it
is necessary to implement containers using linked lists. Each list contains a
block of a few thousand keys. LaMarca notes very little performance difference
between 100 and 5,000 keys in each block. The size is therefore chosen so that
each list fits exactly into a page of memory. When a list is filled, another
list is linked to it, and these lists hold all the keys in the container.

The initial implementation of this allocated a new list every time an old one
was filled. This lowers performance significantly, as it results in a lot of
system calls, which increase instruction count and cache misses. Instead, it is
possible to work out the maximum possible number of lists required to create the
containers, and allocate them at the start. When a list runs out of space, it is
linked to an unused list from the large allocation. This wastes space, since it
is almost guaranteed that less lists will be required than available, though the
number of wasted lists allocated will be less than the number of pivots, and
therefore not exceptionally high.

The entire array is iterated though a key at a time. For each key, the list
of pivots are searched until the appropriate container is found, and the key is
added to that container. Sequentially searching the lists increases the
instruction count by 50\% over the memory-tuned version. For this reason, the
search was replaced with an efficient binary search, which reduced by half the
extra instructions.

Once the entire array has been put into containers, each container is emptied in
turn and the keys put back into the array. When a container is empty, the pivot greater than that
container is put into position, which is guaranteed to be its final position.
The indices of these positions is pushed onto the stack. The stack must
therefore be increased in size before this happens. By pushing the last
array copied onto the stack, an entire set of cache misses is avoided.
Unfortunately, since a multi-partition is only done when the array is
more than twice the size of the cache, this reduction in misses in not
significant.

While emptying the containers back into the array, an opportunity is
taken\footnote{Results do not reflect this step. See \vref{bug1}.} to
find the smallest key in the leftmost partition. This is placed as a sentinel,
ensuring that every partition has a sentinel, and that no bounds checks are
required.

\section{Results}

\subsection{Test Parameters}
The threshold chosen was 10, simply as this is the number used in Sedgewick's
book.

The number of keys in the linked list is 1022 since a word is needed for the
count, and another as a pointer to the next list. 1024 is the number of 32-bit
words which fit in a 4KB page of memory. In earlier versions where each page was
\cc{malloc}ed individually, the number of keys in the linked list was 1018,
as 4 words were used by the particular version of \cc{malloc} used by
SimpleScalar\footnote{This particular \cc{malloc} forms part of
\cc{glibc}, the GNU C library, version 1.09.}.

A 2MB cache was assumed, so the number of keys which fit in the cache for this
in-place sort is 524,288. Multi-quicksort began once the number of keys was
larger than 174,762.

\subsection{Expected Performance}
The instruction count for each quicksort is expected to be $O(NlogN)$,
though not as high as the other algorithms due to the efficient inner loop.
The number of instructions for multi-quicksort should increase by about 50\% in
the case of the sequential sort, and 25\% in the case of the binary sort.

LaMarca notes that quicksort is very cache efficient already. Once it no longer
fits in the cache, however, the number of misses should increase significantly.
This is especially true in the case of the base quicksort, which does an
insertion sort across then entire array at the end.

The number of branches should increase in multi-quicksort along with the
instruction count. When a binary search is used, the increase in misses is
expected to be significant. With a sequential search, however, the number of
misses should not increase at all, and should flat-line from that point on.

\subsection{Simulation Results}

\plot{quick}{}{}{Results shown are for a Direct Mapped cache.}{}{}{}{}

The instruction count, shown in Figure \ref{quick instructions} is as predicted. The expected increases in instruction
count for multi-quicksort was due to a test with a smaller data set and smaller
constants. The actual results were not as high as the expected ones, but closer
to 20\% and 45\% greater, for sequential and binary search versions,
respectively.

The cache results in Figure \ref{quick level 2 misses} were similar to
predictions, except that the multi-quicksorts did not behave as expected, and
performed worse than the memory-tuned version. However, at the edge of the graph
their slopes are considerably less than the slopes of the other sorts, and may
result in lower misses for larger data sets. The tuned version performed better
than the base version, as predicted.

A surprising result is that direct mapped caches often perform better than fully
associative ones. The reason for this is not clear, but it is obvious from the
graph that this is true in the case of all the quicksorts.

The branch prediction results in Figure \ref{quick branch misses} were exactly
as expected. The binary search produced a dramatic increase in the branch
misses, though not when the two-level adaptive predictor was used. A short spike
was recorded in the bimodal results, and its not clear if this is an anomaly or
not. If it is, then the two-level adaptive predictor performs worse than the
bimodal, yet again.

The sequential search stops the increase of branch misses, since every key will
only take one miss to find its proper place. The results of this on cycle count,
shown in Figure \ref{quick cycles} are enough to make up for the increase in
instruction count and level 1 accesses, both of which were significantly reduced
by using the binary search. The tuned quicksort, however, had the lower running
time and even base quicksort out-ran the multi-quicksorts.

LaMarca's results are again similar to the results presented here, but not
identical. The shape of the instruction count graph is the same in both cases,
but that of the cache misses is not. The ratio of the cache misses between base
and cache quicksort are the same in both sets of results, but the number of
cache misses in multi-quicksort is much higher here than it is in LaMarca's
findings. As with mergesort though, while the results are different, the shape
is similar: multi-quicksort has a much flatter slope than the other sorts.  This
difference is likely to be due to the implementation of multi-quicksort; a
slightly different approach, which is described immediately below, may have the
effect of changing the results sufficiently that they may agree with LaMarca's.

\section{Future Work}
In multi-quicksort, it may be possible to reduce the cache misses by sorting
each containerful of keys as soon as it is put back into the array. This step would
significantly increase the temporal locality, and may reduce the number of cache
misses by half.

The threshold in quicksort is the point at which a simpler sort is used to sort
the partition. A larger threshold will results in fewer checks for a sentinel. A
smaller threshold means that the instruction count due to the insertion sort
will be lower, but more partition steps may be taken. A smaller threshold also
makes it more likely that the keys to be sorted all fit in a single cache
block. However, the partition is likely to occur across a cache block boundary,
though this is not necessarily a bad thing; the next partition to be sorted will
be adjacent to the current block, and will already be in memory. Observing how
results, especially instruction count and branch misses, vary with changes 
to the threshold may be interesting.

Two areas of current research into quicksort are those of equal keys, and
median-of-greater-than-3. The former affects the algorithmic properties, and
some steps may need to be varied to avoid extra cache misses as a result
of extra copying. The latter simply affects the instruction count, though the
extra comparisons are likely to be mispredicted. These results could be
measured, giving greater insight to this debate.

The memory-tuned quicksort is the only algorithm here that requires a sentinel
check. The branch predictor will only mispredict this once, so the instruction
count increase is only that of removing a single check which occurs relatively
few ($N / THRESHOLD$)\footnote{This is a minimum. The figure may be higher since
partitions are likely to be smaller than the threshold.} times. Still, the
problem is interesting and has applications in other algorithm designs.

Finally, in multi-quicksort, it may be possible to choose better pivots.
Choosing better (ie, more evenly distributed pivots) would reduce the number of
containers. The advantage of this is that it reduces linearly the number of
pivots to be searched by the sequential search. The most this can be reduced by
is a factor of 3, indicating perfectly distributed pivots. This would reduce the
instruction overhead of the multi-quicksort considerably.
