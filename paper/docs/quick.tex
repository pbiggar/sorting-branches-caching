%\section{Description}

Quicksort is an in-place, unstable sort. It was first described in
\cite{HoarePartition61} and \cite{HoareQuicksort61}, and later in more detail in
\cite{Hoare62}. Sedgewick did his PhD on quicksort, and several important
improvements to quicksort are due to this. They are discussed in
\cite{Sedgewick78} and \cite{Sedgewick02}. \cite{BentleyMcIlroy93} discusses
considerations for a fast C implementation. \cite{SedgewickBentley02} discusses
equal keys and proves that quicksort has the lowest complexity of any comparison
based sort.

Quicksort is an algorithm of the type known as \n{divide and conquer}. It
recursively partitions its array around a pivot, swapping large keys on
the left hand side with smaller keys on the right hand side. The partition
causes the pivot to be put in its final position, with all greater keys to the
right, and lesser keys on the left. The left and right partitions are then
sorted recursively.

The recursive nature of quicksort means that even though quicksort is in-place,
there is a requirement of a stack. In the worst case, this stack can grow to
$O(N)$, if the largest or smallest key is always chosen as the pivot. This
worst case will occur frequently, especially when trying to sort an already
sorted, or almost sorted list.

Figure \vref{Simple quicksort implementation} contains a simple implementation of
the quicksort algorithm.

\code{Simple quicksort implementation}{simple_quicksort.c}

\section{Base Quicksort}

LaMarca uses the optimised quicksort described by Sedgewick for his base
quicksort. This is an iterative quicksort which uses an explicit stack. In the
worst case, this stack must be the same size as the array, and must be
initialised at the start of the algorithm. To reduce the size of the stack,
Sedgewick proposes sorting the smallest partition first. This optimisation
reduces the maximum size of the stack to $O(logN)$. In practice, two \cc{int}s,
the left and the right index, are required for each partition.

Sedgewick's second improvement is the use of an elementary sort for small
subfiles.  Once a partition is smaller than an arbitrary threshold, insertion
sort is used to sort the partition. Sedgewick states that roughly the same
results are obtained for thresholds between 5 and 25. An additional reduction in
instruction count can be had by doing the insertion sort over the entire array
at the end, rather than across each small array as soon as the threshold is
reached. At the end of the partition sorting, LaMarca places a sentinel at the
end of the array and uses an insertion sort, eliminating the bounds check from
the end. In this implementation, the sentinel is placed at the front, and the
smallest of the first \cc{threshold} keys is used as a sentinel, in keeping
with the policy of not exceeding the bounds of the array.

Choosing a pivot which is close to the median of the keys to be sorted ensures
quicksort achieves optimal performance. Hoare suggested a random pivot, but the
added cost of a random number generator would severely slow the algorithm.
Sedgewick recommends that the median of three keys is chosen, the three keys
being the first, middle and last in the list. This reduces the chance of a worst
case occurring significantly, notably in the case of sorted and almost sorted
lists.

An additional benefit of Sedgewick's median of three implementation is that it
provides a sentinel for the partition. It is now possible to remove the bounds
check from the innermost loops of \cc{partition}, saving a lot of instructions.

Between these three improvements, Sedgewick estimates a reduction in
instruction count of roughly 30\%. The size of the auxiliary stack is also
reduced significantly, and the possibility of stack overflow, in the worst case
of the recursive version, is removed.

Quicksort has been well studied, and several important improvements have been
noted. In \cite{BentleyMcIlroy93}, using a median of nine partition is discussed,
and modern discussions on quicksort refer to the problem of equal keys, which is
discussed there, as well as by Sedgewick in \cite{SedgewickBentley02}. These
improvements are not included here for several reasons. Firstly, a quicksort
which takes into account equal keys puts the equal keys at the front of the
array, then copies them into place later. These copies would represent a lot of
cache misses, and the results would overwhelm the improvements LaMarca makes in
the next phase of the algorithm. Secondly, it is not as important to write a
brilliant version of quicksort as it is to analyse LaMarca's improvements.

\section{Memory-Tuned Quicksort}
LaMarca's first step was to increase the temporal locality of the algorithm by
performing insertion sort as soon as the partitioning stops. Once a partition is
reduced below the size of the threshold, insertion sort is preformed
immediately. By doing this, the keys being sorted are in the cache already. By
leaving the insertion sort until the end, as Sedgewick does, the reduction in
instruction count is made up by the increase in cache misses, once the
lists being sorted are larger than the cache. 

As discussed above, it is necessary to insert either a bounds check or a
sentinel.  If the partition to be sorted is the left most partition, then the
smallest key in the partition will be chosen as the sentinel. No other
partition needs a sentinel. A possible way to avoid the sentinel in that
partition is to place zero at $0^{th}$ position in the list to be sorted, and
sort whichever number was in this position back at the end. This sort could be
done with one iteration of insertion sort, and would have just one branch miss.
However, the number of level 2 cache misses would be great in this case.
However, the cost of this would be substantial.  This could be reduced by
choosing the key to remove from a small list, such as the eight keys at the
start, middle and end of the array. This would introduce a constant number of
cache misses, but would significantly reduce the chances of having to sort
across the entire array at the end. This, however, is not implemented; instead a
check is done to fetch a sentinel from the appropriate subfile if required. This
check is expensive, but less so than either a bounds check or putting a sentinel
into every subfile.

\section{Multi-Quicksort}

In the case that the array to be sorted fits in the cache, then quicksort has
excellent cache properties. Once it no longer fits in the cache, however, the
number of misses increases dramatically. To fix this problem, LaMarca uses a
technique similar to that used in multi-mergesort. Instead of multi-merging a
series of sorted lists at the end, a \n{multi-way partition} is done at the start,
moving all keys into cache sized containers which can be sorted individually.

At the start of the algorithm, a set of pivots is chosen and sorted, and keys
are put into containers based on which two pivots their value lies between. A
minimal and maximal key are put on the edges of this list to avoid bounds checks
as the list is iterated over. Because of the way the pivots were chosen, two
pivots which happen to be close together would result in a very small number of
keys being put into their container. While this is not a bad thing, this will
result in a larger number of keys in another container. Conversely, a large
space between two consecutive pivots will mean that the keys in this
container will not all fit into the cache at once.

There are two possible solutions to this. The first is to choose the pivots
well, so that they are very evenly distributed. The second is to choose the
number of pivots so that the average size of the containers is not larger than
the cache. LaMarca presents calculations showing that by choosing the average
subset size to be $C/3$, where $C$ is the number of keys which fit in the
cache\footnote{For a 2MB cache, this is 524,144.}.

Since the maximum number of keys which must fit in a container is not known, it
is necessary to implement containers using linked lists. Each list contains a
block of a few thousand keys. LaMarca notes very little performance difference
between 100 and 5,000 keys in each block. The size is therefore chosen so that
each list fits exactly into a page of memory. When a list is filled, another
list is linked to it, and these lists hold all the keys in the container.

The initial implementation of this allocated a new list every time an old one
was filled. This lowers performance significantly, as it results in a lot of
system calls, which increase instruction count and cache misses. Instead, it is
possible to work out the maximum possible number of lists required to create the
containers, and allocate them at the start. When a list runs out of space, it is
linked to an unused list from the large allocation. This wastes space, since it
is almost guaranteed that less lists will be required than available, though the
number of wasted lists allocated will be less than the number of pivots, and
therefore not exceptionally high.

The entire array is iterated though a key at a time. For each key, the list
of pivots are searched until the appropriate container is found, and the key is
added to that container. Sequentially searching the lists increases the
instruction count by 50\% over the memory-tuned version. For this reason, the
search was replaced with an efficient binary search, which reduced by half the
extra instructions.

Once the entire array has been put into containers, each container is emptied
in turn and the keys put back into the array. When a container is empty, the
pivot greater than that container is put into position, which is guaranteed to
be its final position.  The indices of these positions is pushed onto the
stack, and the subarray is then sorted.

While emptying the containers back into the array, an opportunity is taken to
find the smallest key in the leftmost partition. This is placed as a sentinel,
ensuring that every partition has a sentinel, and that no bounds checks are
required.

\section{Results}

\subsection{Test Parameters}
The threshold chosen was 10, simply as this is the number used in Sedgewick's
book.

The number of keys in the linked list is 1022 since a word is needed for the
count, and another as a pointer to the next list. 1024 is the number of 32-bit
words which fit in a 4KB page of memory. In earlier versions where each page was
\cc{malloc}ed individually, the number of keys in the linked list was 1018,
as 4 words were used by the particular version of \cc{malloc} used by
SimpleScalar\footnote{This particular \cc{malloc} forms part of
\cc{glibc}, the GNU C library, version 1.09.}.

A 2MB cache was assumed, so the number of keys which fit in the cache for this
in-place sort is 524,288. Multi-quicksort began once the number of keys was
larger than 174,762.

\subsection{Expected Performance}
The instruction count for each quicksort is expected to be $O(NlogN)$,
though not as high as the other algorithms due to the efficient inner loop.
The number of instructions for multi-quicksort should increase by about 50\% in
the case of the sequential sort, and 25\% in the case of the binary sort.

LaMarca notes that quicksort is very cache efficient already. Once it no longer
fits in the cache, however, the number of misses should increase significantly.
This is especially true in the case of the base quicksort, which does an
insertion sort across then entire array at the end.

The number of branches should increase in multi-quicksort along with the
instruction count. When a binary search is used, the increase in misses is
expected to be significant. With a sequential search, however, the number of
misses should not increase at all, and should flat-line from that point on.

\subsection{Simulation Results}

\plot{quick}{}{}{Results shown are for a Direct Mapped cache.}{}{}{}{}

The instruction count, shown in Figure \ref{quick instructions} is as
predicted. The expected increases in instruction count for multi-quicksort was
due to a test with a smaller data set and smaller constants. The actual results
were higher than expected: being 63\% and 40\% greater, for
sequential and binary search versions, respectively.
%TODO check

The cache results in Figure \ref{quick level 2 misses} were similar to
predictions, except that the multi-quicksorts did not behave as expected, and
performed worse than the memory-tuned version. Not only do they have more
misses per key, but the rate at which the misses increase with size is also
worse for the multi quicksorts. The tuned version performed better than the
base version, as predicted.

A surprising result is that direct mapped caches often perform better than fully
associative ones. The reason for this is not clear, but it is obvious from the
graph that this is true in the case of all the quicksorts.

The branch prediction results in Figure \ref{quick branch misses} were exactly
as expected. The binary search produced a dramatic increase in the branch
misses, though not when the two-level adaptive predictor was used. A short spike
was recorded in the bimodal results, and its not clear if this is an anomaly or
not. If it is, then the two-level adaptive predictor performs worse than the
bimodal, yet again.

The sequential search stops the increase of branch misses, since every key will
only take one miss to find its proper place. The results of this on cycle count,
shown in Figure \ref{quick cycles} are enough to make up for the increase in
instruction count, which was significantly reduced by using the binary search.
The tuned quicksort, however, had the lower running time and even base
quicksort out-ran the multi-quicksorts.

LaMarca's results are again similar to the results presented here, but not
identical. The shape of the instruction count graph is the same in both cases,
but that of the cache misses is not. The ratio of the cache misses between base
and cache quicksort are the same in both sets of results, but the number of
cache misses in multi-quicksort is much higher here than it is in LaMarca's
findings. In addition, the shape is different. LaMarca's multi-quicksort's
cache misses plateau, while the results here show an increasing slope.

A previous result, however, with a different implementation showed the same
shape, but different results to LaMarca's. That implementation waited until all
the partitioned keys were put back into the array, instead of being sorted
straight away. Those results plateaued once mutli-quicksort took effect.

\section{Predictor results}
\sixplot{base_quicksort-0.eps}{base_quicksort9-0.eps}{cache_quicksort-0.eps}{multi_quicksort-0.eps}{sequential_multi_quicksort-0.eps}{}{base, cache and multi quicksorts and base quicksort9}
\sixplot{base_quicksort1-0.eps}{base_quicksort-0.eps}{base_quicksort5-0.eps}{base_quicksort7-0.eps}{base_quicksort9-0.eps}{}{base quicksort without a median, with median of 3, and with pseudo median of 5, 7 and 9}

%TODO a bit to rewrite

Theres two important bits: comparison between the types of sorts, and comparisons between the different number of things tried as medians.

Firstly, cache quicksort and base quicksort only idffer in how they deal with
the insertion. Base quicksort and base quicksort9 have similar numbers of
insertion branches, and a very similar ratio (2/1 correct/incorrect) of
mispredictions. Comparing base quicksort to cahce quicksort (they both use
median of 3), there is actually a decrease in the number of branches handled by
insertion sort, instead of an increase. An increase is expected (thats why
sedgewick moves the insertion to the end), though when you take into account
the sentinel and the outer loop, there probably is an increase in instructions
at least. Base quicksort has 35\% miss rate, but cache has a 34\%. Not a big
difference, but the point is that there is a reduction in branches, but of the
reduced branches, 50\% were unpredictable. I dont think theres anything
interesting in this fact, however.

When going to multi-quicksort, there si quite a drop in the number fo branches.
Once the sort is larger than the cache, the number fo steps increases linearly
(since it does a sort of counting like radix sort). Theres a decrease of 19 \%.
Since the sort at this point is NlogM where M is the size of the cache (in
keys), there shouldnt be a change in misprediction rates. And there sint.
Before: 63\%. After, 63\%. So no difference.

Theres a lot of branches in the multi quicksort. WE saw in other results that
the mere instruction cost of this bogs down to algorithm significantly. in the
binary search version, there are more than twice as many mispredictions,
despite the fact that there are less than half the branches. (note that i dont
think this was a wonderful binary search, its heavily biased towards the
checking the left). binary search predicts correctly 57 percent of the time,
meaning that it mostly changes direction haphazardly during the search. No
wonder it performs so badly.

Note that sequential multi-quicksort has less mispredictions than cache quicksort.

Part 2: comparing different ways of randomly partitioning:

Ways of getting the partition:
1 - just take the rightmost element. Note that there are no sentinels, so you have to put a bounds check back in
3 - median of(0,1,2) halves
5 - median of(0, (median of(1,2,3), 4) quarters
7 - median of(median of(0,1,2), 3, median of(4,5,6)) sixths
9 - median of(median of(0,1,2), median of(3,4,5), median of(6,7,8)) eights

misprediction rates:
1 - 29.4\%
3 - 35.9\%
5 - 38.4\%
7 - 40.0\%
9 - 41.7\%

The misprediction rates point to the problem. They're just not being evenly divided. The number of times the partitions end is the same, roughly, each time. However, the number of actual branches is much higher in the case of quicksort1, and decreases all the way down. At the same time, the number of mispredictions creeps up. And hence the big change in the rate of mispredictions. As predicted, the better the median, the better the partition, and the less correct predictions pretending to be useful. There must be a 50\% rate waiting to get out, so a little more than median of 9 maybe. Or a lower cut-off point (it cuts out for partitions of less than 40 keys, and uses plain old median of 3).

the rate or number of insertion branches doesnt change at all.


%TODO up to here

\section{Future Work}
The threshold in quicksort is the point at which a simpler sort is used to sort
the partition. A larger threshold will results in fewer checks for a sentinel. A
smaller threshold means that the instruction count due to the insertion sort
will be lower, but more partition steps may be taken. A smaller threshold also
makes it more likely that the keys to be sorted all fit in a single cache
block. However, the partition is likely to occur across a cache block boundary,
though this is not necessarily a bad thing; the next partition to be sorted will
be adjacent to the current block, and will already be in memory. Observing how
results, especially instruction count and branch misses, vary with changes 
to the threshold may be interesting.

Two areas of current research into quicksort are those of equal keys, and
median-of-greater-than-3. The former affects the algorithmic properties, and
some steps may need to be varied to avoid extra cache misses as a result
of extra copying. The latter simply affects the instruction count, though the
extra comparisons are likely to be mispredicted. These results could be
measured, giving greater insight to this debate.

The memory-tuned quicksort is the only algorithm here that requires a sentinel
check. The branch predictor will only mispredict this once, so the instruction
count increase is only that of removing a single check which occurs relatively
few ($N / THRESHOLD$)\footnote{This is a minimum. The figure may be higher since
partitions are likely to be smaller than the threshold.} times. Still, the
problem is interesting and has applications in other algorithm designs.

Finally, in multi-quicksort, it may be possible to choose better pivots.
Choosing better (ie, more evenly distributed pivots) would reduce the number of
containers. The advantage of this is that it reduces linearly the number of
pivots to be searched by the sequential search. The most this can be reduced by
is a factor of 3, indicating perfectly distributed pivots. This would reduce the
instruction overhead of the multi-quicksort considerably.
