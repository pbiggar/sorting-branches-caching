\label{method}
This section discusses the method and framework used to complete this research.
The first section discusses the steps involved in creating a testing framework,
programming the sorts and getting results. The second section discusses tools
that I used or created to do this.

\section{Method}

The following were the objectives of the research:
\begin{itemize}
\item Implement heapsort, mergesort, quicksort, radixsort, shellsort and the four
elementary sorts: insertion sort, selection sort, bubblesort and shakersort.
\item Simulate these sorts using the SimpleScalar architecture simulator, using a
variety of branch predictors and cache configurations.
\item Implement cache-conscious sorts from \cite{LaMarca96} and
\cite{LaMarca99}.
\item Investigate and analyse the increase in speed, reduction in cache misses, 
and change in branch prediction accuracy as a result of these changes.
\item Determine if any changes can be made to improved the sorts' performance.
\end{itemize}

% TODO maybe remoe this?
Several steps were necessary to prepare for the research.  A framework was
required by which sorts could be tested as they were being developed. This is
discussed in Section \ref{Main}. It was also necessary to fully understand the
technologies to be used in the project. Several texts were used for this: cache
and branch predictors are both in \cite{HennessyPatterson90}; branch predictors
in \cite{Uht97}, as well as in \cite{McFarling93} and \cite{Smith81}; sorts are
discussed in \cite{Sedgewick02} and \cite{Knuth98}; cache-conscious sorts are
discussed \cite{LaMarca96}, \cite{LaMarca99} and \cite{LaMarcaHeap96}.

There were many steps involved in writing each iteration of the algorithms.
Insertion and selection sort had just one of these iterations; the other
sorts had more than one\footnote{Quicksort, for example, had four: base
quicksort, tuned quicksort, multi-quicksort and sequential multi-quicksort.}. A
flowchart of these steps appears in Figure \vref{Flowchart}.

%TODO this is going to have to be updated
\begin{figure}
\scalebox{0.57}{\includegraphics*[0pt,165pt][600pt,802pt]{flowchart}}
\caption{Flowchart describing the development of an algorithm}
\label{Flowchart}
\end{figure}


The first step in each case was to write the algorithm. \cite{Sedgewick02} was the primary
source of these, though in several cases LaMarca specified a particular version
which he used. When a step involved in an algorithm wasn't clear, the paper
describing the algorithm was used to clarify the meaning.

Once the algorithm was written, a testing framework was used. This tested for
edge-cases in the algorithms by testing sorts sized from $8$ to $255$ keys, with
different random data sets. Testing was also performed on a very large data set.
This test was timed and the results were considered. The results of this test
were not accurate: the program took up to a minute to run, during which time the
computer was being used by multiple users and processes, leading to context
switches, sharing of resources and scheduling. Despite these inaccuracies, it was
possible to use the results relative to each other, and to determine whether,
for example, memory-tuned quicksort was running faster than base quicksort.

Early in the project, it was decided that exceeding the bounds of an array
during a sort was to be forbidden. No serious sorting library or application
could acceptably work in this manner, and any results obtained while allowing
this were only of academic interest. It isn't possible, from a normal run of a
program, to determine if an arrays bounds are exceeded slightly, though exceeding
the bounds by a large amount will result is a segmentation fault. Because of
this, Valgrind (see Section \ref{Valgrind}), was used to report these errors.

Once it was certain that a sort actually sorted without error, it was necessary
to ensure that this was not a fluke occurrence. Many of the sorts had multiple
passes, and if one pass did not properly sort the array segment it was meant to,
the array might still be sorted by the next pass, though potentially much more
slowly. To test that this did not occur, a set of functions to visually display
arrays and variables at certain points was developed, which created a HTML page
showing the progression of the algorithm. An example of this is in Figure
\vref{Visual Sort}.

\begin{figure}
\scalebox{0.58}{\includegraphics*{visual}}
\caption{A sample visual sort routine, showing a merge from the source array to
the auxiliary array}
\label{Visual Sort}
\end{figure}

This shows two stages of a bitonic mergesort which is described in Section \ref{base
mergesort}. The top of the image shows the source array being merged into the
target array below. The source array shows three large segments of the array and
four smaller segments. The first two large segments have been merged into one
segment in the target array. The third of these has been merged into the
corresponding target array segment. The first of the smaller segments has begun
being merged with the other small segments. The rest of the target array is
uninitialised.  Below that is a small amount of debugging information. $i == j$
is a key point in the program and is printed as debugging information. In the
two arrays below this, the merge continues.

While it was not known in advance the desired instruction count of an algorithm,
a certain range was implied by LaMarca's descriptions. The relationship between
one iteration of an algorithm, or between base versions of several algorithms
was known ahead of time. One configuration of the \cc{simple} script ran quick
tests using the \cc{sim-fast} simulator, giving instruction counts which could
be compared to other versions. 

If both these tests and the visual display of the sort's behaviour seemed
correct, then the full collection of simulations was run. The results of the
simulation were compared to results of other versions and to the expected
results. If the results were surprising, then it was necessary to explain the
reasons for this. If the cause was a bug, then the bug was fixed and the tests
repeated.

The results were then turned into graphs, from which conclusions about the
performance of the algorithm could be reached, and patterns could be spotted.
More details of this are in Section \ref{Gnuplot}. The next iteration of the
algorithm was then begun, and the process repeated for each of the algorithm's
iterations.

\section{Tools}

\subsection{SimpleScalar} \cc{SimpleScalar} is an architecture simulator. It
works by compiling code into its native instruction set, PISA, then emulating a
processor as it runs the program.  SimpleScalar can be found at
\n{http://www.simplescalar.com}. Documentation on how to use and modify
SimpleScalar are \cite{Burger97}, \cite{Austin02}, \cite{Tutorialv4},
\cite{Tutorialv2} and \cite{SimpleScalarUserGuide}.

SimpleScalar works by compiling code into its own instruction set. It provides a
compiler - a port of gcc 2.63 - with related tools such as a linker and
assembler.  Several simulators are then provided: \cc{sim-fast} only counts
instruction, quickly; \cc{sim-cache} simulates a cache and counts level 1 and
level 2 cache misses; \cc{sim-bpred} simulates a branch predictor and counts
branch hits and misses. Other simulators are provided, but they were not used in
this project.

The output of SimpleScalar is human readable, but is not conducive to collecting
large amounts of data. To that end, a script was written to run each required
set of simulations. It recorded data from the simulations into a single file for
later retrieval. This file was suitable to reading with a \cc{diff} program.

The SimpleScalar settings were chosen to be similar to LaMarca's simulations.
LaMarca used the \n{Atom} simulator, which was used to test software for the DEC
AlphaStation. It simulated a 2MB cache with a 32 byte cache line. The remainder
of the settings were taken from the DEC Alpha: 8KB level 1 data cache, 8KB level
1 instruction cache and shared instruction and data level 2 cache. 

\subsection{Gnuplot}
\label{Gnuplot}
\cc{Gnuplot} is a tool for plotting data files. It provides visualisations of data in
two and three dimensions, and is used to create all of the graphs in this
document. Its homepage is \n{http://www.gnuplot.info/}.

There were several steps involved in using gnuplot for the data. Firstly, the
data was not in the correct format for gnuplot, and had to be converted. It also
needed to be offset by the cost of filling the array with random variables, and
scaled to so that the y-axis could be in the form of \n{plotted data per key},
both to be more readable, and to be consistent with LaMarca's graphs. Once the data
was converted, it was necessary to generate a gnuplot script for each graph.
The number of graphs required made this an
impossible manual task, and so another script had to be written.  Some graphs
still needed to be tweaked by hand, due to difficulties in the script, but the
task was much shortened by this, and the tweaking was mostly accomplished using
macros in the development platform.

Finally, a script to compare the data together was required.  This compared each
sort against each other sort in key metrics, such a branch or cache misses. It
also compared each cache size against other cache sizes for a particular sort,
and branch predictors against other branch predictors for each sort. 

\subsection{Main}
\label{Main}

The main program provided the framework for the project. Imaginatively called
\cc{main.c}, this program provided the means for testing and (roughly)
timing each sort. Figure \vref{Key sections of Main.c} contains this code.

\code{Key sections of Main.c}{main.c}

\subsection{Valgrind}
\label{Valgrind}
\cc{Valgrind} is a set of tools which are used for debugging and profiling.  By
default it works as a memory checker, and detects errors such as the use of
uninitialised memory and accessing memory off the end of an array. This use
makes it particularly useful for bounds checking. No configuration was required
for this, and simply running \cc{valgrind a.out} ran the program through
valgrind. The homepage for valgrind is at \n{http://valgrind.kde.org/}.

\subsection{perfex}
\cc{Perfex} is a tool used to access performance counters on a Pentium 4. By default,
it prints out the number of cycles\footnote{The processor automatically
maintains this figure in a 40-bit register called P4\_TSC (Time Stamp Counter),
and perfex prints this value after running the program} which a program takes to
execute. This was used to create the graphs comparing the running time of sorts.
The gnuplot scripts for this were short and easily written.

Perfex was an alternative to using SimpleScalar. By using its hardware
registers, actual real world results could have been measured. Using perfex
instead of SimpleScalar was an option reviewed at the start, but the difficulty
involved in setting up a machine to use it was considerable. Additionally,
because perfex measures results on a real machine, only one type of cache and
branch predictor could have been measured. Since access was
provided late in the project, limited measurements were taken to supplement the
SimpleScalar results.

\section{Implementation of sorts}

To show an example of the creation of a sort, mergesort will now be discussed.
Descriptions of some of the terms here is saved for discussion in Chapter
\ref{merge}.

The base mergesort described by LaMarca is Algorithm N. Three improvements
were prescribed by LaMarca: changing the sort to be bitonic to eliminate bounds
checks, unrolling the inner loop, and pre-sorting the array with a simple in-place
sort. He also recommended making the sort pass between two arrays to avoid
unnecessary copying.

Algorithm N is described in \cite{Knuth98}. A flow control diagram and step by
step guide is provided, but very little detail of how the sort actually works is
included. Therefore, the first step was to understand how this sort worked. This
was done by first analysing the flow control of the algorithm. The steps of the
sort are described in assembly, and while they can be written in C, the result
is a very low-level version. I attempted to rewrite the algorithm without
\cc{goto} statements and labels, but instead using standard flow control
statements such as \cc{if}, \cc{while} and \cc{do-while}. Despite the time spent
on this, it was impossible to faithfully reproduce the behaviour without
duplicating code.

As a result, it was necessary to use the original version, and to try to perform
optimisations on it. However, several factors made this difficult or impossible.
LaMarca unrolled the inner loop of mergesort, but it is not clear which is the
inner loop. The two deepest loops were executed very infrequently, and
unrolling them had no effect on the instruction count. Unrolling the other loops
also had no effect, as it was rare for the loop to execute more than five or six
times, and once or twice was more usual.

Pre-sorting the array was also not exceptionally useful. The instruction count
actually increased slightly as a result of this, indicating that algorithm N
does a better job of pre-sorting the array than the inlined pre-sort did.

Algorithm N does not have a bounds check. It treats the entire array
bitonically; that is, rather than have pairs of array beings merged together,
the entire array is slowly merged inwards, with the right hand side being sorted
in descending order and the left hand side being sorted in ascending order.
Since there is no bounds check, there is no bounds check to be eliminated.

As a result of this problems, algorithm S, from the same source, was
investigated. This behaved far more like LaMarca's description: LaMarca
described his base mergesort as sorting into lists of size two, then four and
so on. Algorithm S behaves in this manner, though Algorithm N does not. Fully
investigating algorithm S, which is described in the same manner as algorithm N,
was deemed to be too time-consuming, despite its good performance.

As a result, a mergesort was written partially based on the outline of algorithm
N. The outer loop was very similar, but the number of merges required were
calculated in advance, and the indices of each merge step.  It was very simple
to turn this into a bitonic array; adding pre-sorting reduced the instruction
count, as did unrolling the loop. The reduction in instruction count from the
initial version, which performed the same number of instructions as the same as
algorithm N, and the optimised version, was about 30\%.

A tiled mergesort was then written, based on LaMarca's guidance. Firstly, this
was aligned in memory so that the number of misses due to conflicts would be
reduced.  Then it fully sorted segments of the array, each half the size of the
cache, so that temporal locality would be exploited. Finally, all these segments
were merged together in the remaining merge steps.

LaMarca's final step was to create a multi-mergesort. This attempted to reduce
the number of cache misses in the final merge steps of tiled mergesort by
merging all the sorted arrays in one step. This required the use of an 8-heap,
which had been developed as part of heapsort.

Once these sorts were written it was attempted to improve them slightly.
Instead of a tiled mergesort, a double tiled mergesort was written.  This
attempted to completely sort in the level 1 cache, then to completely sort in
the level 2 cache. Double multi-mergesort was a combination of this and of
multi-mergesort. The rewrite was based on the base mergesort, and was written
far more cleanly, being more readable and efficient. These were also found to
reduce the instruction count, as well as the level 2 cache miss count. These
results are discussed in more detail in Section \ref{Mergesort results}.

Each of these sort variations had to be fully written from the descriptions in
LaMarca's thesis. Only the base algorithm was available from textbooks, and this
needed to be extensively analysed and rewritten. Each step which was written had
to be debugged and simulated. The results were compared with the expected
results and frequently the sorts needed to be edited to reduce the cost of a
feature.

%TODO another sentence here
Each of the four $O(NlogN)$ sorts and radixsort, was treated in this fashion.

\section{Future work}
An improvement that would have yielded interesting results would be to do simulations
which only consisted of flow control, with no comparisons. This would allow a
division of flow control misses and comparative misses. However, this would not
always be possible; sorts like quicksort and heapsort use their data to
control the flow of the program. In addition, bubblesort, shakersort and
selection sort have (analytically) predictable flow control, while radixsort has
no comparison misses at all. Mergesort may benefit from this, however, especially versions
with difficult flow control diagrams, such as \n{algorithm N} and \n{algorithm
S} (see Section \ref{Algorithm N}).

A metric not considered so far is the number of instructions between branches.
This should be an important metric, and investigating it could point to
important properties within an algorithm.

Valgrind comes with a tool called \cc{cachegrind}, which is a cache
profiler, detailing where cache misses occur in a program. Using this on a
sort may indicate areas for potential improvement.

As well as the number of cycles, Pentium 4 performance registers can also provide
most of the same information as SimpleScalar, with different parameters. For
example, the cache line of a Pentium 4 machine is 128 bytes whereas 32 byte
cache lines were used in SimpleScalar.  The addition of real world data would
strengthen the results from this project.
